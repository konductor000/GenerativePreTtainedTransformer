{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, BertTokenizer\n",
    "import torch.optim as optim\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from dataset.create_dataset import BookGenerator, create_test_dataset, create_test\n",
    "from layers.model import Transformer, AutoregressiveWrapper\n",
    "from test_model.test_model import TestModel\n",
    "\n",
    "import wandb\n",
    "import yadisk\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#!git clone https://github.com/konductor000/GenerativePretrainedTransformer\\n!pip install -r requirements.txt\\n!pip install rouge_score\\n!MAX_JOBS=4 pip install flash-attn --no-build-isolation\\n!pip install datasets\\n!pip install transformers\\n!pip install evaluate\\n!pip install spacy\\n!pip install yadisk\\n!pip install wandb\\n!python -m spacy download en_core_web_md\\n#8a49fefdd8a82ca9ba659a874b09adf8c5995778\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#!git clone https://github.com/konductor000/GenerativePretrainedTransformer\n",
    "!pip install -r requirements.txt\n",
    "!pip install rouge_score\n",
    "!MAX_JOBS=4 pip install flash-attn --no-build-isolation\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install evaluate\n",
    "!pip install spacy\n",
    "!pip install yadisk\n",
    "!pip install wandb\n",
    "!python -m spacy download en_core_web_md\n",
    "#8a49fefdd8a82ca9ba659a874b09adf8c5995778\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    \"architecture\": \"Transformer\", # Wandb only\n",
    "    \"dataset\": \"books\", #\"wikitext-103-raw-v1\", # Wandb only\n",
    "    \"batch_size\": 16,\n",
    "    \"embedding_size\": 768,\n",
    "    \"max_sequence_length\": 1024,\n",
    "    \"number_of_layers\": 12,\n",
    "    \"number_of_heads\": 12,\n",
    "    \"extention_factor\": 4,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    'test_size': 1024,\n",
    "    'start_book': 0,\n",
    "    \"test_every\": 256,\n",
    "    \"log_traing_metrics_every\": 16,\n",
    "    'flash_atten': True,\n",
    "    'num_train_books': 2500,\n",
    "    'save_every': 120,\n",
    "    'use_mixed_precision': True,\n",
    "    'model_path': None, # \"savepoints/dulcet-serenity-218\",\n",
    "    \n",
    "}\n",
    "CONFIG[\"lr\"] = 0.001 / np.sqrt(CONFIG[\"batch_size\"])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def log_metrics(test_name, test_dataloader, pipeline, model_tester, train_config, CONFIG):\n",
    "    pipeline.eval()\n",
    "    if test_name in ['books']:\n",
    "        metrics = model_tester.test_model(pipeline, test_dataloader)\n",
    "        test_loss = metrics['loss']\n",
    "        bleu = metrics['bleu']\n",
    "        rouge1 = metrics['rouge1']\n",
    "        rouge2 = metrics['rouge2']\n",
    "        rougeL = metrics['rougeL']\n",
    "\n",
    "        wandb.log({\n",
    "            \"test_loss_\" + test_name: test_loss,\n",
    "            \"bleu_\" + test_name: bleu,\n",
    "            \"rouge1_\" + test_name: rouge1,\n",
    "            \"rouge2_\" + test_name: rouge2,\n",
    "            \"rougeL_\" + test_name: rougeL,\n",
    "        }) \n",
    "    else:\n",
    "        simmilarity_accuracy, simmilarity_score = model_tester.test_simmilarity(pipeline,\n",
    "                                                            test_dataloader, tokenizer)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"simmilarity_accuracy\": simmilarity_accuracy,\n",
    "            \"simmilarity_score\": simmilarity_score,\n",
    "        })\n",
    "\n",
    "    pipeline.train()\n",
    "        \n",
    "\n",
    "def train(CONFIG, pipeline, model, optimizer, loss_function, model_tester, wandb,\n",
    "          wandb_run, scaler, y, dataloaders):\n",
    "    book_generator, test_book_dataloader, cloze_dataloader = dataloaders\n",
    "\n",
    "    train_time = 0\n",
    "    total_train_time = time()\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    batch_num = 0\n",
    "    train_losses = []\n",
    "    tests = ['books', 'simmilarity_score']\n",
    "    test_dataloaders = [test_book_dataloader, cloze_dataloader]\n",
    "    \n",
    "    for i in tqdm(range(CONFIG['num_train_books']), desc=\"Training Progress\"):\n",
    "        train_dataloader = book_generator.next_book(10)\n",
    "        if time() - total_train_time > CONFIG['save_every'] * 60:\n",
    "            total_train_time = time()\n",
    "            save_model(model, wandb_run, y)\n",
    "            \n",
    "        for batch in train_dataloader:\n",
    "            train_start = time()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                model_output, target = pipeline(input_ids, attention_mask)\n",
    "                loss = loss_function(model_output.transpose(1, 2), target)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            train_time += time() - train_start\n",
    "            batch_num += 1\n",
    "    \n",
    "            if batch_num % CONFIG[\"log_traing_metrics_every\"] == 0:\n",
    "                wandb.log({\n",
    "                    \"train_loss\": sum(train_losses[-CONFIG[\"log_traing_metrics_every\"]:]) / CONFIG[\"log_traing_metrics_every\"],\n",
    "                    \"train_time\": train_time / CONFIG[\"log_traing_metrics_every\"],\n",
    "                })\n",
    "                train_time = 0\n",
    "            \n",
    "            if batch_num % CONFIG[\"test_every\"] == 0:\n",
    "                test_start = time()\n",
    "                for i in range(len(tests)):\n",
    "                    log_metrics(tests[i], test_dataloaders[i], pipeline, model_tester, train_config, CONFIG)\n",
    "                wandb.log({\n",
    "                    \"test_time\": time() - test_start,\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(CONFIG, tokenizer, model_path=None):\n",
    "    number_of_tokens = tokenizer.vocab_size\n",
    "    \n",
    "    y = yadisk.YaDisk(token=\"y0_AgAAAAA7vZKNAADLWwAAAADsK4dQ-f3fKT3hSianJggcCcKC1Mfxo8s\")\n",
    "    print(y.check_token())\n",
    "    model = Transformer(\n",
    "        embedding_size=CONFIG[\"embedding_size\"],\n",
    "        number_of_tokens=number_of_tokens,\n",
    "        number_of_heads=CONFIG[\"number_of_heads\"],\n",
    "        number_of_layers=CONFIG[\"number_of_layers\"],\n",
    "        extention_factor=CONFIG[\"extention_factor\"],\n",
    "        dropout_rate=CONFIG[\"dropout_rate\"],\n",
    "        max_sequence_length=CONFIG[\"max_sequence_length\"],\n",
    "        use_flash_att=CONFIG[\"flash_atten\"],\n",
    "    ).to(device)\n",
    "    if model_path:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    pipeline = AutoregressiveWrapper(model).to(device)\n",
    "    loss_function = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
    "    model_tester = TestModel(tokenizer, model)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    return pipeline, model, optimizer, loss_function, model_tester, scaler, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, wandb_run, y):\n",
    "    if not os.path.exists('save_models'):\n",
    "        os.makedirs('save_models')\n",
    "    save_dir = f\"save_models/{wandb_run.name}\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    saved_models = [f for f in os.listdir(save_dir) if f.startswith(\"model_\")]\n",
    "    num_saved_models = len(saved_models)\n",
    "\n",
    "    model_filename = f\"model_{num_saved_models + 1}\"\n",
    "    model_path = f'{save_dir}/{model_filename}'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    try:\n",
    "        y.mkdir(save_dir)\n",
    "    except yadisk.exceptions.PathExistsError:\n",
    "        pass\n",
    "    print(model_path)\n",
    "    y.upload(model_path, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskorodumov-work\u001b[0m (\u001b[33m8667\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c866793440f4dcfbde8b952883a9368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011114448032134936, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/GenerativePretrainedTransformer/wandb/run-20230915_173909-mnp39iwv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/8667/transformer/runs/mnp39iwv' target=\"_blank\">fresh-field-357</a></strong> to <a href='https://wandb.ai/8667/transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/8667/transformer' target=\"_blank\">https://wandb.ai/8667/transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/8667/transformer/runs/mnp39iwv' target=\"_blank\">https://wandb.ai/8667/transformer/runs/mnp39iwv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_run = wandb.init(\n",
    "    project=\"transformer\",\n",
    "    tags=[\"long_training_testing\"],\n",
    "    #id=\"sblota90\",\n",
    "    resume=\"allow\",\n",
    "    config=CONFIG\n",
    ")\n",
    "\n",
    "load_path = CONFIG['model_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77ba4f6561f649a29688231a51298983",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "number of parameters = 129622852\n",
      "number of trainable parameters = 129622852\n",
      "memory allocated in GB = 0.4828827530145645\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "book_generator = BookGenerator(batch_size=CONFIG[\"batch_size\"],\n",
    "                                max_sequence_length=CONFIG[\"max_sequence_length\"], tokenizer=tokenizer, num_workers=4)\n",
    "test_dataloader = create_test(32, 512, tokenizer=tokenizer, dataset_size=CONFIG['test_size'])\n",
    "book_generator.skip_books(CONFIG['start_book'])\n",
    "cloze_dataset = create_test_dataset('data.csv')\n",
    "\n",
    "pipeline, model, optimizer, loss_function, model_tester, scaler, y = create_model(CONFIG, tokenizer, CONFIG[\"model_path\"])\n",
    "num_parameters, num_trainable_parameters, memory_allocated = pipeline.count_parameters() \n",
    "print('number of parameters =', num_parameters)\n",
    "print('number of trainable parameters =', num_trainable_parameters)\n",
    "print('memory allocated in GB =', memory_allocated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(CONFIG, pipeline, model, optimizer, loss_function, model_tester, wandb, wandb_run, scaler, y,\n",
    "     [book_generator, test_dataloader, cloze_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, wandb_run, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Mount Everest is the highest peak in\n",
    "\"\"\"\n",
    "num_predicted_tokens = 20\n",
    "k = 3\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "\n",
    "start = time()\n",
    "with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "    answer = pipeline.predict_next(input_text, tokenizer, num_predicted_tokens, k)\n",
    "print(time() - start)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def predict_next(pipeline, input_text, num_predicted_tokens, tokenizer):\n",
    "    input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    input_tokens = input_tokens[:, :-1].to(device)\n",
    "    \n",
    "    for i in range(num_predicted_tokens):\n",
    "        mask = torch.ones_like(input_tokens)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probabilities = pipeline.next_token_probabilities(input_tokens, mask)\n",
    "        \n",
    "        answer = probabilities.argsort(dim=-1)[:, -randint(1, 3)].unsqueeze(0)\n",
    "        input_tokens = torch.cat((input_tokens, answer), dim=1)\n",
    "        \n",
    "    return tokenizer.decode(input_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data time - 11.215234994888306\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "\n",
    "data_prep_timings = []\n",
    "model_train_timings = []\n",
    "gradient_timings = []\n",
    "times1 = []\n",
    "times2 = []\n",
    "times3 = []\n",
    "times4 = []\n",
    "\n",
    "train_dataloader = book_generator.next_book(10)\n",
    "print('data time -', time() - start)\n",
    "full_train_time = time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full time - 58.213600158691406\n",
      "number of batches - 136\n",
      "data_prep_timings - 1.0260894298553467\n",
      "model_train_timings - 2.477931499481201\n",
      "gradient_timings - 42.88483500480652\n",
      "backward - 41.26165699958801\n",
      "clipping - 0.5091650485992432\n",
      "step - 1.1016998291015625\n",
      "updating - 0.01080322265625\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    data_prep_time = time()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "    data_prep_timings.append(time() - data_prep_time)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    model_time = time()\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        model_output, target = pipeline(input_ids, attention_mask)\n",
    "        loss = loss_function(model_output.transpose(1, 2), target)\n",
    "\n",
    "    model_train_timings.append(time() - model_time)\n",
    "\n",
    "    grad_time = time()\n",
    "    time1 = time()\n",
    "    scaler.scale(loss).backward()\n",
    "    time2 = time()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    time3 = time()\n",
    "    scaler.step(optimizer)\n",
    "    time4 = time()\n",
    "    scaler.update()\n",
    "    times1.append(time2-time1)\n",
    "    times2.append(time3-time2)\n",
    "    times3.append(time4-time3)\n",
    "    times4.append(time()-time4)\n",
    "    gradient_timings.append(time() - grad_time)\n",
    "\n",
    "print('full time -', time() - start)\n",
    "print('number of batches -', len(train_dataloader))\n",
    "print('data_prep_timings -', sum(data_prep_timings))\n",
    "print('model_train_timings -', sum(model_train_timings))\n",
    "print('gradient_timings -', sum(gradient_timings))\n",
    "print('backward -', sum(times1))\n",
    "print('clipping -', sum(times2))\n",
    "print('step -', sum(times3))\n",
    "print('updating -', sum(times4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-09-15 18:07:51 8238:8238 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-09-15 18:07:51 8238:8238 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-09-15 18:07:51 8238:8238 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        train_dataloader = book_generator.next_book(1)\n",
    "        for batch in train_dataloader:\n",
    "            train_start = time()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            #optimizer.zero_grad()\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                model_output, target = pipeline(input_ids, attention_mask)\n",
    "                #loss = loss_function(model_output.transpose(1, 2), target)\n",
    "            \n",
    "            #scaler.scale(loss).backward()\n",
    "            #scaler.step(optimizer)\n",
    "            #scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference        62.05%     268.058ms        99.99%     431.941ms     431.941ms       0.000us         0.00%      54.725ms      54.725ms             1  \n",
      "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...        20.78%      89.777ms        20.80%      89.829ms      44.914ms       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                               aten::to         0.90%       3.901ms         9.57%      41.320ms     317.846us       0.000us         0.00%       2.500ms      19.231us           130  \n",
      "                                           aten::linear         0.22%     963.000us         8.92%      38.547ms     393.337us       0.000us         0.00%      80.141ms     817.765us            98  \n",
      "                                         aten::_to_copy         0.33%       1.413ms         8.79%      37.989ms     303.912us       0.000us         0.00%       2.703ms      21.624us           125  \n",
      "                                            aten::copy_         1.16%       5.021ms         7.21%      31.159ms     247.294us       2.706ms         4.94%       2.706ms      21.476us           126  \n",
      "                                        cudaMemcpyAsync         5.69%      24.574ms         5.69%      24.574ms      12.287ms       0.000us         0.00%       0.000us       0.000us             2  \n",
      "                                            aten::addmm         1.68%       7.254ms         2.51%      10.825ms     220.918us      39.451ms        72.09%      39.451ms     805.122us            49  \n",
      "                                    aten::empty_strided         1.66%       7.174ms         1.66%       7.174ms      44.559us       0.000us         0.00%       0.000us       0.000us           161  \n",
      "                                       aten::layer_norm         0.20%     856.000us         1.53%       6.593ms     235.464us       0.000us         0.00%       2.562ms      91.500us            28  \n",
      "                                            aten::empty         1.37%       5.934ms         1.37%       5.934ms      38.038us       0.000us         0.00%       0.000us       0.000us           156  \n",
      "                                aten::native_layer_norm         0.53%       2.285ms         1.33%       5.737ms     229.480us       2.562ms         4.68%       2.562ms     102.480us            25  \n",
      "                                 FlashAttnQKVPackedFunc         0.59%       2.551ms         1.11%       4.786ms     398.833us       4.524ms         8.27%       4.524ms     377.000us            12  \n",
      "                                       cudaLaunchKernel         0.79%       3.425ms         0.79%       3.425ms      13.173us       0.000us         0.00%       0.000us       0.000us           260  \n",
      "                                          aten::dropout         0.01%      44.000us         0.62%       2.665ms     222.083us       0.000us         0.00%     750.000us      62.500us            12  \n",
      "                                   aten::native_dropout         0.29%       1.265ms         0.61%       2.621ms     218.417us     750.000us         1.37%     750.000us      62.500us            12  \n",
      "                                              aten::add         0.45%       1.951ms         0.53%       2.283ms      91.320us       2.632ms         4.81%       2.632ms     105.280us            25  \n",
      "                                       aten::empty_like         0.03%     126.000us         0.40%       1.721ms      46.514us       0.000us         0.00%       0.000us       0.000us            37  \n",
      "                                             aten::relu         0.07%     318.000us         0.31%       1.340ms     111.667us       0.000us         0.00%       2.003ms     166.917us            12  \n",
      "                                        aten::clamp_min         0.20%     862.000us         0.24%       1.022ms      85.167us       2.003ms         3.66%       2.003ms     166.917us            12  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 431.973ms\n",
      "Self CUDA time total: 54.725ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "for input_text in data['Sentence']:\n",
    "    num_predicted_tokens = 40\n",
    "    \n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        answer = predict_next(pipeline, input_text, num_predicted_tokens, tokenizer)\n",
    "\n",
    "    print('input:', input_text)\n",
    "    print('output:', answer[len(input_text)+6:])\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y.download(\"save_models/ethereal-planet-258/model_1\", \"model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in data['Sentence']:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------ ------------ ------------\n",
      "                          Name    CPU total   CUDA total \n",
      "------------------------------ ------------ ------------\n",
      "               model_inference       1.252s    248.518ms\n",
      "e_function: EmbeddingBackwa...    237.355ms     10.034ms\n",
      "            EmbeddingBackward0    237.281ms     10.034ms\n",
      "      aten::embedding_backward    237.269ms     10.034ms\n",
      "aten::embedding_dense_backward    237.259ms     10.034ms\n",
      "         cudaStreamSynchronize    233.147ms      0.000us\n",
      "ltiProcessingDataLoaderIter...     96.446ms      0.000us\n",
      "                  aten::linear     87.562ms    200.274ms\n",
      "                      aten::to     83.526ms     39.836ms\n",
      "                aten::_to_copy     75.562ms     41.652ms\n",
      "      Optimizer.step#Adam.step     68.165ms     27.202ms\n",
      "           aten::empty_strided     64.171ms      0.000us\n",
      "luate_function: AddmmBackward0     61.350ms    212.371ms\n",
      "e_function: ToCopyBackward0...     40.085ms     15.214ms\n",
      "                   aten::copy_     40.077ms     68.415ms\n",
      "                AddmmBackward0     38.429ms    186.552ms\n",
      "                      aten::mm     34.365ms    186.552ms\n",
      "               ToCopyBackward0     33.630ms     15.023ms\n",
      "              cudaLaunchKernel     26.357ms     41.979ms\n",
      "                   aten::empty     25.413ms      0.000us\n",
      "------------------------------ ------------ ------------\n",
      "Self CPU time total: 1.663s\n",
      "Self CUDA time total: 553.424ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
    "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
    "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
    "                                        model_inference        62.05%     268.058ms        99.99%     431.941ms     431.941ms       0.000us         0.00%      54.725ms      54.725ms             1  \n",
    "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...        20.78%      89.777ms        20.80%      89.829ms      44.914ms       0.000us         0.00%       0.000us       0.000us             2  \n",
    "                                               aten::to         0.90%       3.901ms         9.57%      41.320ms     317.846us       0.000us         0.00%       2.500ms      19.231us           130  \n",
    "                                           aten::linear         0.22%     963.000us         8.92%      38.547ms     393.337us       0.000us         0.00%      80.141ms     817.765us            98  \n",
    "                                         aten::_to_copy         0.33%       1.413ms         8.79%      37.989ms     303.912us       0.000us         0.00%       2.703ms      21.624us           125  \n",
    "                                            aten::copy_         1.16%       5.021ms         7.21%      31.159ms     247.294us       2.706ms         4.94%       2.706ms      21.476us           126  \n",
    "                                        cudaMemcpyAsync         5.69%      24.574ms         5.69%      24.574ms      12.287ms       0.000us         0.00%       0.000us       0.000us             2  \n",
    "                                            aten::addmm         1.68%       7.254ms         2.51%      10.825ms     220.918us      39.451ms        72.09%      39.451ms     805.122us            49  \n",
    "                                    aten::empty_strided         1.66%       7.174ms         1.66%       7.174ms      44.559us       0.000us         0.00%       0.000us       0.000us           161  \n",
    "                                       aten::layer_norm         0.20%     856.000us         1.53%       6.593ms     235.464us       0.000us         0.00%       2.562ms      91.500us            28  \n",
    "                                            aten::empty         1.37%       5.934ms         1.37%       5.934ms      38.038us       0.000us         0.00%       0.000us       0.000us           156  \n",
    "                                aten::native_layer_norm         0.53%       2.285ms         1.33%       5.737ms     229.480us       2.562ms         4.68%       2.562ms     102.480us            25  \n",
    "                                 FlashAttnQKVPackedFunc         0.59%       2.551ms         1.11%       4.786ms     398.833us       4.524ms         8.27%       4.524ms     377.000us            12  \n",
    "                                       cudaLaunchKernel         0.79%       3.425ms         0.79%       3.425ms      13.173us       0.000us         0.00%       0.000us       0.000us           260  \n",
    "                                          aten::dropout         0.01%      44.000us         0.62%       2.665ms     222.083us       0.000us         0.00%     750.000us      62.500us            12  \n",
    "                                   aten::native_dropout         0.29%       1.265ms         0.61%       2.621ms     218.417us     750.000us         1.37%     750.000us      62.500us            12  \n",
    "                                              aten::add         0.45%       1.951ms         0.53%       2.283ms      91.320us       2.632ms         4.81%       2.632ms     105.280us            25  \n",
    "                                       aten::empty_like         0.03%     126.000us         0.40%       1.721ms      46.514us       0.000us         0.00%       0.000us       0.000us            37  \n",
    "                                             aten::relu         0.07%     318.000us         0.31%       1.340ms     111.667us       0.000us         0.00%       2.003ms     166.917us            12  \n",
    "                                        aten::clamp_min         0.20%     862.000us         0.24%       1.022ms      85.167us       2.003ms         3.66%       2.003ms     166.917us            12  \n",
    "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
    "Self CPU time total: 431.973ms\n",
    "Self CUDA time total: 54.725ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                                        CPU        CUDA\n",
    "model_inference       1.252s    248.518ms\n",
    "e_function: EmbeddingBackwa...    237.355ms     10.034ms\n",
    "EmbeddingBackward0    237.281ms     10.034ms\n",
    "aten::embedding_backward    237.269ms     10.034ms\n",
    "aten::embedding_dense_backward    237.259ms     10.034ms\n",
    "cudaStreamSynchronize    233.147ms      0.000us\n",
    "ltiProcessingDataLoaderIter...     96.446ms      0.000us\n",
    "aten::linear     87.562ms    200.274ms\n",
    "aten::to     83.526ms     39.836ms\n",
    "aten::_to_copy     75.562ms     41.652ms\n",
    "Optimizer.step#Adam.step     68.165ms     27.202ms\n",
    "aten::empty_strided     64.171ms      0.000us\n",
    "luate_function: AddmmBackward0     61.350ms    212.371ms\n",
    "e_function: ToCopyBackward0...     40.085ms     15.214ms\n",
    "aten::copy_     40.077ms     68.415ms\n",
    "AddmmBackward0     38.429ms    186.552ms\n",
    "aten::mm     34.365ms    186.552ms\n",
    "ToCopyBackward0     33.630ms     15.023ms\n",
    "cudaLaunchKernel     26.357ms     41.979ms\n",
    "aten::empty     25.413ms      0.000us"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "8defd53a528e9245923bfdc9d5a38f5c3ca09cbc6e92fa1b95a37aeffbb04cd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
