{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "import torch.optim as optim\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from dataset.create_dataset import BookGenerator, create_test_dataset, create_test\n",
    "from layers.model import Transformer, AutoregressiveWrapper\n",
    "from test_model.test_model import TestModel\n",
    "\n",
    "import wandb\n",
    "import yadisk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#!git clone https://github.com/konductor000/GenerativePretrainedTransformer\\n!pip install -r requirements.txt\\n!pip install rouge_score\\n!MAX_JOBS=4 pip install flash-attn --no-build-isolation\\n!pip install datasets\\n!pip install transformers\\n!pip install evaluate\\n!pip install spacy\\n!pip install yadisk\\n!pip install wandb\\n#8a49fefdd8a82ca9ba659a874b09adf8c5995778\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#!git clone https://github.com/konductor000/GenerativePretrainedTransformer\n",
    "!pip install -r requirements.txt\n",
    "!pip install rouge_score\n",
    "!MAX_JOBS=4 pip install flash-attn --no-build-isolation\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install evaluate\n",
    "!pip install spacy\n",
    "!pip install yadisk\n",
    "!pip install wandb\n",
    "#8a49fefdd8a82ca9ba659a874b09adf8c5995778\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    \"architecture\": \"Transformer\", # Wandb only\n",
    "    \"dataset\": \"books\", #\"wikitext-103-raw-v1\", # Wandb only\n",
    "    \"batch_size\": 24,\n",
    "    \"embedding_size\": 768,\n",
    "    \"max_sequence_length\": 1024,\n",
    "    \"number_of_layers\": 12,\n",
    "    \"number_of_heads\": 12,\n",
    "    \"extention_factor\": 4,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    'test_size': 1024,\n",
    "    'start_book': 0,\n",
    "    \"test_every\": 256,\n",
    "    \"log_traing_metrics_every\": 16,\n",
    "    'flash_atten': True,\n",
    "    'num_train_books': 2500,\n",
    "    'save_every': 120,\n",
    "    'use_mixed_precision': True,\n",
    "    'model_path': None, # \"savepoints/dulcet-serenity-218\",\n",
    "    \n",
    "}\n",
    "CONFIG[\"lr\"] = 0.001 / np.sqrt(CONFIG[\"batch_size\"])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def log_metrics(test_name, test_dataloader, pipeline, model_tester, train_config, CONFIG):\n",
    "    pipeline.eval()\n",
    "    if test_name in ['books']:\n",
    "        metrics = model_tester.test_model(pipeline, test_dataloader)\n",
    "        test_loss = metrics['loss']\n",
    "        bleu = metrics['bleu']\n",
    "        rouge1 = metrics['rouge1']\n",
    "        rouge2 = metrics['rouge2']\n",
    "        rougeL = metrics['rougeL']\n",
    "\n",
    "        wandb.log({\n",
    "            \"test_loss_\" + test_name: test_loss,\n",
    "            \"bleu_\" + test_name: bleu,\n",
    "            \"rouge1_\" + test_name: rouge1,\n",
    "            \"rouge2_\" + test_name: rouge2,\n",
    "            \"rougeL_\" + test_name: rougeL,\n",
    "        }) \n",
    "    else:\n",
    "        simmilarity_accuracy, simmilarity_score = model_tester.test_simmilarity(pipeline,\n",
    "                                                            test_dataloader, tokenizer)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"simmilarity_accuracy\": simmilarity_accuracy,\n",
    "            \"simmilarity_score\": simmilarity_score,\n",
    "        })\n",
    "\n",
    "    pipeline.train()\n",
    "        \n",
    "\n",
    "def train(CONFIG, pipeline, model, optimizer, loss_function, model_tester, wandb,\n",
    "          wandb_run, scaler, y, dataloaders):\n",
    "    train_config = {\n",
    "        \"test_every\": 4096 // CONFIG[\"batch_size\"],\n",
    "        \"log_traing_metrics_every\": 64 // CONFIG[\"batch_size\"],\n",
    "    }\n",
    "    \n",
    "    book_generator, test_book_dataloader, cloze_dataloader = dataloaders\n",
    "\n",
    "    train_time = 0\n",
    "    total_train_time = time()\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    batch_num = 0\n",
    "    train_losses = []\n",
    "    tests = ['books', 'simmilarity_score']\n",
    "    test_dataloaders = [test_book_dataloader, cloze_dataloader]\n",
    "    \n",
    "    for i in tqdm(range(CONFIG['num_train_books']), desc=\"Training Progress\"):\n",
    "        train_dataloader = book_generator.next_book(10)\n",
    "        if time() - total_train_time > CONFIG['save_every'] * 60:\n",
    "            total_train_time = time()\n",
    "            save_model(model, wandb_run, y)\n",
    "            \n",
    "        for batch in train_dataloader:\n",
    "            train_start = time()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                model_output, target = pipeline(input_ids, attention_mask)\n",
    "                loss = loss_function(model_output.transpose(1, 2), target)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            train_time += time() - train_start\n",
    "            batch_num += 1\n",
    "    \n",
    "            if batch_num % CONFIG[\"log_traing_metrics_every\"] == 0:\n",
    "                wandb.log({\n",
    "                    \"train_loss\": sum(train_losses[-CONFIG[\"log_traing_metrics_every\"]:]) / train_config[\"log_traing_metrics_every\"],\n",
    "                    \"train_time\": train_time / CONFIG[\"log_traing_metrics_every\"],\n",
    "                })\n",
    "                train_time = 0\n",
    "            \n",
    "            if batch_num % CONFIG[\"test_every\"] == 0:\n",
    "                test_start = time()\n",
    "                for i in range(len(tests)):\n",
    "                    log_metrics(tests[i], test_dataloaders[i], pipeline, model_tester, train_config, CONFIG)\n",
    "                wandb.log({\n",
    "                    \"test_time\": time() - test_start,\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(CONFIG, model_path=None):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-16B-mono\")\n",
    "    number_of_tokens = tokenizer.vocab_size\n",
    "    \n",
    "    y = yadisk.YaDisk(token=\"y0_AgAAAAA7vZKNAADLWwAAAADsK4dQ-f3fKT3hSianJggcCcKC1Mfxo8s\")\n",
    "    print(y.check_token())\n",
    "    model = Transformer(\n",
    "        embedding_size=CONFIG[\"embedding_size\"],\n",
    "        number_of_tokens=number_of_tokens,\n",
    "        number_of_heads=CONFIG[\"number_of_heads\"],\n",
    "        number_of_layers=CONFIG[\"number_of_layers\"],\n",
    "        extention_factor=CONFIG[\"extention_factor\"],\n",
    "        additional_feed_forward_layers=CONFIG[\"additional_feed_forward_layers\"],\n",
    "        dropout_rate=CONFIG[\"dropout_rate\"],\n",
    "        max_sequence_length=CONFIG[\"max_sequence_length\"],\n",
    "        use_flash_att=CONFIG[\"flash_atten\"],\n",
    "    ).to(device)\n",
    "    if model_path:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    pipeline = AutoregressiveWrapper(model).to(device)\n",
    "    loss_function = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
    "    model_tester = TestModel(tokenizer, model)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    return pipeline, model, optimizer, loss_function, model_tester, scaler, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, wandb_run, y):\n",
    "    if not os.path.exists('save_models'):\n",
    "        os.makedirs('save_models')\n",
    "    save_dir = f\"save_models/{wandb_run.name}\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    saved_models = [f for f in os.listdir(save_dir) if f.startswith(\"model_\")]\n",
    "    num_saved_models = len(saved_models)\n",
    "\n",
    "    model_filename = f\"model_{num_saved_models + 1}\"\n",
    "    model_path = f'{save_dir}/{model_filename}'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    try:\n",
    "        y.mkdir(save_dir)\n",
    "    except yadisk.exceptions.PathExistsError:\n",
    "        pass\n",
    "    print(model_path)\n",
    "    y.upload(model_path, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskorodumov-work\u001b[0m (\u001b[33m8667\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/GenerativePretrainedTransformer/wandb/run-20230912_193750-ar0uiyds</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/8667/transformer/runs/ar0uiyds' target=\"_blank\">earthy-butterfly-310</a></strong> to <a href='https://wandb.ai/8667/transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/8667/transformer' target=\"_blank\">https://wandb.ai/8667/transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/8667/transformer/runs/ar0uiyds' target=\"_blank\">https://wandb.ai/8667/transformer/runs/ar0uiyds</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_run = wandb.init(\n",
    "    project=\"transformer\",\n",
    "    tags=[\"long_training_testing\"],\n",
    "    #id=\"sblota90\",\n",
    "    resume=\"allow\",\n",
    "    config=CONFIG\n",
    ")\n",
    "\n",
    "load_path = CONFIG['model_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb2315e9027e44959c748c06d7aa9a9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Collecting en-core-web-md==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.6.0/en_core_web_md-3.6.0-py3-none-any.whl (42.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /opt/conda/lib/python3.10/site-packages (from en-core-web-md==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.5)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.24.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (6.4.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.29.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (23.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (65.6.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /opt/conda/lib/python3.10/site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (4.7.1)\n",
      "Requirement already satisfied: pydantic-core==2.6.3 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.6.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (0.1.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (8.1.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-md==3.6.0) (2.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n",
      "number of parameters = 131968314\n",
      "number of trainable parameters = 131968314\n",
      "memory allocated in GB = 0.4916202798485756\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-16B-mono\")\n",
    "book_generator = BookGenerator(batch_size=CONFIG[\"batch_size\"],\n",
    "                                max_sequence_length=CONFIG[\"max_sequence_length\"], tokenizer=tokenizer)\n",
    "test_dataloader = create_test(32, 512, tokenizer=tokenizer, dataset_size=CONFIG['test_size'])\n",
    "book_generator.skip_books(CONFIG['start_book'])\n",
    "cloze_dataset = create_test_dataset('data.csv')\n",
    "\n",
    "pipeline, model, optimizer, loss_function, model_tester, scaler, y = create_model(CONFIG, CONFIG[\"model_path\"])\n",
    "num_parameters, num_trainable_parameters, memory_allocated = pipeline.count_parameters() \n",
    "print('number of parameters =', num_parameters)\n",
    "print('number of trainable parameters =', num_trainable_parameters)\n",
    "print('memory allocated in GB =', memory_allocated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  12%|█▏        | 300/2500 [1:59:55<11:00:03, 18.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_models/earthy-butterfly-310/model_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  25%|██▍       | 622/2500 [3:58:37<11:48:33, 22.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_models/earthy-butterfly-310/model_2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  37%|███▋      | 919/2500 [6:00:08<12:32:04, 28.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_models/earthy-butterfly-310/model_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  49%|████▊     | 1214/2500 [8:00:21<11:16:21, 31.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_models/earthy-butterfly-310/model_4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  59%|█████▉    | 1485/2500 [10:00:37<7:17:37, 25.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_models/earthy-butterfly-310/model_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  70%|███████   | 1758/2500 [12:00:44<4:33:41, 22.13s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_models/earthy-butterfly-310/model_6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:  71%|███████   | 1776/2500 [12:10:27<4:57:46, 24.68s/it]\n"
     ]
    },
    {
     "ename": "ClientResponseError",
     "evalue": "500, message='Internal Server Error', url=URL('https://data.together.xyz/redpajama-data-1T/v1.0.0/book/book.jsonl')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientResponseError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_tester\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m     \u001b[49m\u001b[43m[\u001b[49m\u001b[43mbook_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcloze_dataset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 50\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(CONFIG, pipeline, model, optimizer, loss_function, model_tester, wandb, wandb_run, scaler, y, dataloaders)\u001b[0m\n\u001b[1;32m     47\u001b[0m test_dataloaders \u001b[38;5;241m=\u001b[39m [test_book_dataloader, cloze_dataloader]\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_train_books\u001b[39m\u001b[38;5;124m'\u001b[39m]), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Progress\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 50\u001b[0m     train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mbook_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnext_book\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m time() \u001b[38;5;241m-\u001b[39m total_train_time \u001b[38;5;241m>\u001b[39m CONFIG[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msave_every\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m:\n\u001b[1;32m     52\u001b[0m         total_train_time \u001b[38;5;241m=\u001b[39m time()\n",
      "File \u001b[0;32m/GenerativePretrainedTransformer/dataset/create_dataset.py:67\u001b[0m, in \u001b[0;36mBookGenerator.next_book\u001b[0;34m(self, num_books)\u001b[0m\n\u001b[1;32m     65\u001b[0m books \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_books):\n\u001b[0;32m---> 67\u001b[0m     book \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     68\u001b[0m     books\u001b[38;5;241m.\u001b[39mappend(book)\n\u001b[1;32m     70\u001b[0m book_parts \u001b[38;5;241m=\u001b[39m split_book_into_parts(books, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_sequence_length)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/iterable_dataset.py:1379\u001b[0m, in \u001b[0;36mIterableDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1376\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m formatter\u001b[38;5;241m.\u001b[39mformat_row(pa_table)\n\u001b[1;32m   1377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m-> 1379\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, example \u001b[38;5;129;01min\u001b[39;00m ex_iterable:\n\u001b[1;32m   1380\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures:\n\u001b[1;32m   1381\u001b[0m         \u001b[38;5;66;03m# `IterableDataset` automatically fills missing columns with None.\u001b[39;00m\n\u001b[1;32m   1382\u001b[0m         \u001b[38;5;66;03m# This is done with `_apply_feature_types_on_example`.\u001b[39;00m\n\u001b[1;32m   1383\u001b[0m         example \u001b[38;5;241m=\u001b[39m _apply_feature_types_on_example(\n\u001b[1;32m   1384\u001b[0m             example, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures, token_per_repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_token_per_repo_id\n\u001b[1;32m   1385\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/datasets/iterable_dataset.py:233\u001b[0m, in \u001b[0;36mExamplesIterable.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 233\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_examples_fn(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/togethercomputer--RedPajama-Data-1T/d71c21cfe3a11386de9f82d60d553adc57848a1f1a4117b170d80b8fc866dc0e/RedPajama-Data-1T.py:197\u001b[0m, in \u001b[0;36mRedPajama1T._generate_examples\u001b[0;34m(self, files)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m files[subset]:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 197\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(f):\n\u001b[1;32m    198\u001b[0m             \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    199\u001b[0m                 data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(row)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/implementations/http.py:600\u001b[0m, in \u001b[0;36mHTTPFile.read\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    599\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc, length)\n\u001b[0;32m--> 600\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/spec.py:1790\u001b[0m, in \u001b[0;36mAbstractBufferedFile.read\u001b[0;34m(self, length)\u001b[0m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1788\u001b[0m     \u001b[38;5;66;03m# don't even bother calling fetch\u001b[39;00m\n\u001b[1;32m   1789\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1790\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fetch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(out)\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/caching.py:396\u001b[0m, in \u001b[0;36mBytesCache._fetch\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m=\u001b[39m start\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 396\u001b[0m         new \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetcher\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m+\u001b[39m new\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/asyn.py:121\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m obj \u001b[38;5;129;01mor\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msync\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/asyn.py:106\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m FSTimeoutError \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreturn_result\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(return_result, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m return_result\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m return_result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/asyn.py:61\u001b[0m, in \u001b[0;36m_runner\u001b[0;34m(event, coro, result, timeout)\u001b[0m\n\u001b[1;32m     59\u001b[0m     coro \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mwait_for(coro, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m coro\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[1;32m     63\u001b[0m     result[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m ex\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/fsspec/implementations/http.py:655\u001b[0m, in \u001b[0;36mHTTPFile.async_fetch_range\u001b[0;34m(self, start, end)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m r\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m416\u001b[39m:\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;66;03m# range request outside file\u001b[39;00m\n\u001b[1;32m    654\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 655\u001b[0m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# If the server has handled the range request, it should reply\u001b[39;00m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;66;03m# with status 206 (partial content). But we'll guess that a suitable\u001b[39;00m\n\u001b[1;32m    659\u001b[0m \u001b[38;5;66;03m# Content-Range header or a Content-Length no more than the\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \u001b[38;5;66;03m# requested range also mean we have got the desired range.\u001b[39;00m\n\u001b[1;32m    661\u001b[0m response_is_range \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    662\u001b[0m     r\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m206\u001b[39m\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parse_content_range(r\u001b[38;5;241m.\u001b[39mheaders)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m start\n\u001b[1;32m    664\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mint\u001b[39m(r\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Length\u001b[39m\u001b[38;5;124m\"\u001b[39m, end \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m end \u001b[38;5;241m-\u001b[39m start\n\u001b[1;32m    665\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/aiohttp/client_reqrep.py:1005\u001b[0m, in \u001b[0;36mClientResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m-> 1005\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ClientResponseError(\n\u001b[1;32m   1006\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_info,\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory,\n\u001b[1;32m   1008\u001b[0m     status\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m   1009\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreason,\n\u001b[1;32m   1010\u001b[0m     headers\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m   1011\u001b[0m )\n",
      "\u001b[0;31mClientResponseError\u001b[0m: 500, message='Internal Server Error', url=URL('https://data.together.xyz/redpajama-data-1T/v1.0.0/book/book.jsonl')"
     ]
    }
   ],
   "source": [
    "train(CONFIG, pipeline, model, optimizer, loss_function, model_tester, wandb, wandb_run, scaler, y,\n",
    "     [book_generator, test_dataloader, cloze_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_models/earthy-butterfly-310/model_7\n"
     ]
    }
   ],
   "source": [
    "save_model(model, wandb_run, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3774392604827881\n",
      "[CLS] mount everest is the highest peak in the world, the highest peak in the world, the highest peak in the world, the highest peak\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"\n",
    "Mount Everest is the highest peak in\n",
    "\"\"\"\n",
    "num_predicted_tokens = 20\n",
    "k = 3\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-16B-mono\")\n",
    "\n",
    "start = time()\n",
    "with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "    answer = pipeline.predict_next(input_text, tokenizer, num_predicted_tokens, k)\n",
    "print(time() - start)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def predict_next(pipeline, input_text, num_predicted_tokens, tokenizer):\n",
    "    input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    input_tokens = input_tokens[:, :-1].to(device)\n",
    "    \n",
    "    for i in range(num_predicted_tokens):\n",
    "        mask = torch.ones_like(input_tokens)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probabilities = pipeline.next_token_probabilities(input_tokens, mask)\n",
    "        \n",
    "        answer = probabilities.argsort(dim=-1)[:, -randint(1, 3)].unsqueeze(0)\n",
    "        input_tokens = torch.cat((input_tokens, answer), dim=1)\n",
    "        \n",
    "    return tokenizer.decode(input_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: london is the capital of\n",
      "output:  the united states. [ sidenotes : _ american revolution. _ - the american revolution of 1848 was a failure of a revolution of 1688, in which it took its first place in europe. it is\n",
      "----------------------------------------\n",
      "input: The Eiffel Tower is located in\n",
      "output:  a hollow, and is surrounded with walls of the usual form, with walls, and windows, with windows. it has no trace, except the walls and walls of the castle, which were once covered\n",
      "----------------------------------------\n",
      "input: The Amazon River flows through several countries including\n",
      "output:  new zealand, and in many other directions is a tributary from new guinea, and from the north to new south - eastern new guinea. [ illustration ] the new guinea coast and its islands lie to the\n",
      "----------------------------------------\n",
      "input: The Amazon River flows through several countries, isn't\n",
      "output:  the only country in europe, but is the only country in europe which can afford to be the most important of all the great lakes in asia minor. it was in these countries and in europe and in\n",
      "----------------------------------------\n",
      "input: The Declaration of Independence was signed in\n",
      "output:  the council of the republic, and the government of great britain was to take steps for its support. it had no other object than the acquisition of territory by the british crown, and it had no object\n",
      "----------------------------------------\n",
      "input: The Sahara Desert is located in\n",
      "output:  the south - eastern corner. in south - east australia the australian bushmen of australia are said not unworthy of notice, as the bush - dwellers are said to do, and the bushmen are\n",
      "----------------------------------------\n",
      "input: The big building where you learn new things and meet friends is a\n",
      "output:  very good thing. it is a great thing to be a good one, to make oneself a better man than to be an unlucki and a good man, but not so bad. you must\n",
      "----------------------------------------\n",
      "input: When you want to travel around the city, you can take a colorful car called a\n",
      "output:  \" _ la bona _ \" or \" _ de la paixe, a lycopode _ \" or _ la bona de la _, \" or a _ de leur\n",
      "----------------------------------------\n",
      "input: Dogs are known for their loyalty and\n",
      "output:  loyalty, and for the love which the king had of the queen, and for the loyalty and loyalty which she bore, they were the first of them to be sacrificed. [ 6 : _ a '\n",
      "----------------------------------------\n",
      "input: The sun sets in the\n",
      "output:  west and west, the moon rises in the east, and is in the west. it was the first day that i saw it, and i was glad to find that it had been so long since\n",
      "----------------------------------------\n",
      "input: Owls are nocturnal birds, which means they are most active\n",
      "output: , but they do not fly, as the birds are not infringing upon them. the bird of paradise is a species that is very rarely mistaken. birds, birds and other animals have a very\n",
      "----------------------------------------\n",
      "input: Abraham Lincoln was the President of the United States during the\n",
      "output:  war. the president of his party had a large army in his army. the army was commanded to march, 1864, and it was the first time in his life that lincoln was so far successful.\n",
      "----------------------------------------\n",
      "input: Mount Everest is the highest peak in\n",
      "output:  all his history. the peak, the highest mountain, has the peak of the peak, which has the summits, and the summit, and the summit, of the peak. it was in that\n",
      "----------------------------------------\n",
      "input: Music has the power to evoke strong\n",
      "output:  feelings in favour with the people. it was the custom of the church to make an impression on all who had any influence on the people. the people, it will readily admit that the people of england\n",
      "----------------------------------------\n",
      "input: The Great Barrier Reef is a famous natural wonder located in\n",
      "output:  this part. the great gulf is called _ koya'ko'koya _, \" the great gulf of finland. \" it is not only the most remarkable of all these islands in the history\n",
      "----------------------------------------\n",
      "input: The Earth orbits around the\n",
      "output:  sun are not the sun, nor are there any signs of the sun ; the earth, as well it is, does contain a great mass of matter ; the moon, when the sun is set in\n",
      "----------------------------------------\n",
      "input: Honey is produced by\n",
      "output:  david edwards. the projector of this work is a man of the first importance, as a man who has the highest aspirations in human nature. it will be noticed in passing through the introduction of mr\n",
      "----------------------------------------\n",
      "input: When you're feeling unwell, it's important to get plenty of\n",
      "output:  air. \" the doctor had been sitting in his chair, and he looked as though his eyes would have been turned to the other world, but he did see that the doctor was still sitting in a\n",
      "----------------------------------------\n",
      "input: A caterpillar transforms into a\n",
      "output:  moth. the moth is a caterpi of medium diameter, with an outer surface. the caterer'd moth is not a caterer, but a caterer. in the pupal state the\n",
      "----------------------------------------\n",
      "input: The moon is Earth's natural\n",
      "output:  sphere of life, the earth, and man is not the creator. the moon, the sun, and man are all alike. the moon is a natural and natural sphere. man's nature is\n",
      "----------------------------------------\n",
      "input: In cold weather, water can freeze and become\n",
      "output:  frozen in a few minutes ; the air is so heated and heated as it is to be heated by the sun's hot heat, which is a most cooling and refreshing drink. the water is so\n",
      "----------------------------------------\n",
      "input: Many birds migrate to warmer areas during the\n",
      "output:  winter season ; the summer of this month was a busy season for them ; the winter of that summer of the following summer was marked by the summer of the same month, which had already become a summer\n",
      "----------------------------------------\n",
      "input: The Statue of Liberty holds a\n",
      "output:  place among all those in power. the statue is the most conspicuous figure among the christian churches. it is a very good specimen of a christian church, and the most interesting of its churches. its principal\n",
      "----------------------------------------\n",
      "input: The capital city of Japan is\n",
      "output:  the most important of the three great divisions of the world. it has a most imposing appearance. its streets and streets are lined in all directions. its streets are paved and the pavement is filled up by\n",
      "----------------------------------------\n",
      "input: The sun rises in the\n",
      "output:  east, the sky becomes red, and in it is seen a bright gleam. \" it seems that this is a very good reason, for the sun is shining on the sky ; but it seems that\n",
      "----------------------------------------\n",
      "input: Flowers need sunlight and water to\n",
      "output:  be kept in the room ; the sun dance is to begin, and it must not interfere in any part with its proper season, but with a view, if not, at any time to the summer\n",
      "----------------------------------------\n",
      "input: Fish live in\n",
      "output:  a large and well cultivated district of the pacific, and the natives of central asia, and other islands, have a great reputation in this country. it is not the least important of these that the indians\n",
      "----------------------------------------\n",
      "input: People wear coats in\n",
      "output:  which to wear trousers. \" \" i've got to be very particular, mr collins. but, mr jones?'tis not the first time that you have been here. \" mr collins looked\n",
      "----------------------------------------\n",
      "input: The sky is usually\n",
      "output:  so dark as not, as a rule of the earth, so dark as that which covers it. but it is the very shadow of the great mystery which has so far led to its solution. it\n",
      "----------------------------------------\n",
      "input: Water is essential for\n",
      "output:  a large amount to keep up a large circulation. it is essential to the formation of a great mass. in this respect the great masses have not only the power, the capacity of the earth, the\n",
      "----------------------------------------\n",
      "input: The moon shines at\n",
      "output:  the zenith of her light. \" \" but how is she to get the moon, and how is the sun? is it to get the moon? and if she does not get it to do it\n",
      "----------------------------------------\n",
      "input: Apples can be green, or\n",
      "output:  other wild fruits, and they will have a sweet odor, as it will be. _ g'o.'_ - - _ k ` ` `'_ - _ ` _ - - a\n",
      "----------------------------------------\n",
      "input: Milk comes from\n",
      "output:  the mountains, but the indians do it. the indians, who had no medicine, were not willing, for they were willing to go with their friends and help the sick. the indians, who knew\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "for input_text in data['Sentence']:\n",
    "    num_predicted_tokens = 40\n",
    "    \n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        answer = predict_next(pipeline, input_text, num_predicted_tokens, tokenizer)\n",
    "\n",
    "    print('input:', input_text)\n",
    "    print('output:', answer[len(input_text)+6:])\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y.download(\"save_models/ethereal-planet-258/model_1\", \"model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Possible Answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>london is the capital of</td>\n",
       "      <td>Great Britain, Britain, England</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Eiffel Tower is located in</td>\n",
       "      <td>Paris, France</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Amazon River flows through several countri...</td>\n",
       "      <td>Brazil, Peru, Colombia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Amazon River flows through several countri...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Declaration of Independence was signed in</td>\n",
       "      <td>1776, Philadelphia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Sahara Desert is located in</td>\n",
       "      <td>Africa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The big building where you learn new things an...</td>\n",
       "      <td>school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>When you want to travel around the city, you c...</td>\n",
       "      <td>taxi, bus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dogs are known for their loyalty and</td>\n",
       "      <td>friendliness, faithfulness, companionship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The sun sets in the</td>\n",
       "      <td>evening, west, horizon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Owls are nocturnal birds, which means they are...</td>\n",
       "      <td>at night, during darkness, in darkness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Abraham Lincoln was the President of the Unite...</td>\n",
       "      <td>Civil War, 1860s, war period</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mount Everest is the highest peak in</td>\n",
       "      <td>the world, Himalayas, Earth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Music has the power to evoke strong</td>\n",
       "      <td>emotions, feelings, reactions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The Great Barrier Reef is a famous natural won...</td>\n",
       "      <td>Australia, the Pacific Ocean, Queensland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The Earth orbits around the</td>\n",
       "      <td>Sun, star, center of the solar system, solar s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Honey is produced by</td>\n",
       "      <td>bees, worker bees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>When you're feeling unwell, it's important to ...</td>\n",
       "      <td>rest, sleep, relaxation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A caterpillar transforms into a</td>\n",
       "      <td>butterfly, winged insect, beautiful flying cre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The moon is Earth's natural</td>\n",
       "      <td>satellite, companion, celestial body</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>In cold weather, water can freeze and become</td>\n",
       "      <td>ice, solid, frozen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Many birds migrate to warmer areas during the</td>\n",
       "      <td>winter, cold season, chilly months</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>The Statue of Liberty holds a</td>\n",
       "      <td>torch, flaming torch, lit torch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>The capital city of Japan is</td>\n",
       "      <td>Tokyo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>The sun rises in the</td>\n",
       "      <td>east, morning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Flowers need sunlight and water to</td>\n",
       "      <td>grow, bloom, flourish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Fish live in</td>\n",
       "      <td>water, ocean, sea, lake, river</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>People wear coats in</td>\n",
       "      <td>winter, cold weather, chilly seasons</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>The sky is usually</td>\n",
       "      <td>blue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Water is essential for</td>\n",
       "      <td>life</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>The moon shines at</td>\n",
       "      <td>night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Apples can be green, or</td>\n",
       "      <td>red, yellow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Milk comes from</td>\n",
       "      <td>cows</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Sentence  \\\n",
       "0                            london is the capital of   \n",
       "1                      The Eiffel Tower is located in   \n",
       "2   The Amazon River flows through several countri...   \n",
       "3   The Amazon River flows through several countri...   \n",
       "4       The Declaration of Independence was signed in   \n",
       "5                     The Sahara Desert is located in   \n",
       "6   The big building where you learn new things an...   \n",
       "7   When you want to travel around the city, you c...   \n",
       "8                Dogs are known for their loyalty and   \n",
       "9                                 The sun sets in the   \n",
       "10  Owls are nocturnal birds, which means they are...   \n",
       "11  Abraham Lincoln was the President of the Unite...   \n",
       "12               Mount Everest is the highest peak in   \n",
       "13                Music has the power to evoke strong   \n",
       "14  The Great Barrier Reef is a famous natural won...   \n",
       "15                        The Earth orbits around the   \n",
       "16                               Honey is produced by   \n",
       "17  When you're feeling unwell, it's important to ...   \n",
       "18                    A caterpillar transforms into a   \n",
       "19                        The moon is Earth's natural   \n",
       "20       In cold weather, water can freeze and become   \n",
       "21      Many birds migrate to warmer areas during the   \n",
       "22                      The Statue of Liberty holds a   \n",
       "23                       The capital city of Japan is   \n",
       "24                               The sun rises in the   \n",
       "25                 Flowers need sunlight and water to   \n",
       "26                                       Fish live in   \n",
       "27                               People wear coats in   \n",
       "28                                 The sky is usually   \n",
       "29                             Water is essential for   \n",
       "30                                 The moon shines at   \n",
       "31                            Apples can be green, or   \n",
       "32                                    Milk comes from   \n",
       "\n",
       "                                     Possible Answers  \n",
       "0                     Great Britain, Britain, England  \n",
       "1                                       Paris, France  \n",
       "2                              Brazil, Peru, Colombia  \n",
       "3                                                  it  \n",
       "4                                  1776, Philadelphia  \n",
       "5                                              Africa  \n",
       "6                                              school  \n",
       "7                                           taxi, bus  \n",
       "8           friendliness, faithfulness, companionship  \n",
       "9                              evening, west, horizon  \n",
       "10             at night, during darkness, in darkness  \n",
       "11                       Civil War, 1860s, war period  \n",
       "12                        the world, Himalayas, Earth  \n",
       "13                      emotions, feelings, reactions  \n",
       "14           Australia, the Pacific Ocean, Queensland  \n",
       "15  Sun, star, center of the solar system, solar s...  \n",
       "16                                  bees, worker bees  \n",
       "17                            rest, sleep, relaxation  \n",
       "18  butterfly, winged insect, beautiful flying cre...  \n",
       "19               satellite, companion, celestial body  \n",
       "20                                 ice, solid, frozen  \n",
       "21                 winter, cold season, chilly months  \n",
       "22                    torch, flaming torch, lit torch  \n",
       "23                                              Tokyo  \n",
       "24                                      east, morning  \n",
       "25                              grow, bloom, flourish  \n",
       "26                     water, ocean, sea, lake, river  \n",
       "27               winter, cold weather, chilly seasons  \n",
       "28                                               blue  \n",
       "29                                               life  \n",
       "30                                              night  \n",
       "31                                        red, yellow  \n",
       "32                                               cows  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "london is the capital of\n",
      "The Eiffel Tower is located in\n",
      "The Amazon River flows through several countries including\n",
      "The Amazon River flows through several countries, isn't\n",
      "The Declaration of Independence was signed in\n",
      "The Sahara Desert is located in\n",
      "The big building where you learn new things and meet friends is a\n",
      "When you want to travel around the city, you can take a colorful car called a\n",
      "Dogs are known for their loyalty and\n",
      "The sun sets in the\n",
      "Owls are nocturnal birds, which means they are most active\n",
      "Abraham Lincoln was the President of the United States during the\n",
      "Mount Everest is the highest peak in\n",
      "Music has the power to evoke strong\n",
      "The Great Barrier Reef is a famous natural wonder located in\n",
      "The Earth orbits around the\n",
      "Honey is produced by\n",
      "When you're feeling unwell, it's important to get plenty of\n",
      "A caterpillar transforms into a\n",
      "The moon is Earth's natural\n",
      "In cold weather, water can freeze and become\n",
      "Many birds migrate to warmer areas during the\n",
      "The Statue of Liberty holds a\n",
      "The capital city of Japan is\n",
      "The sun rises in the\n",
      "Flowers need sunlight and water to\n",
      "Fish live in\n",
      "People wear coats in\n",
      "The sky is usually\n",
      "Water is essential for\n",
      "The moon shines at\n",
      "Apples can be green, or\n",
      "Milk comes from\n"
     ]
    }
   ],
   "source": [
    "for text in data['Sentence']:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "8defd53a528e9245923bfdc9d5a38f5c3ca09cbc6e92fa1b95a37aeffbb04cd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
