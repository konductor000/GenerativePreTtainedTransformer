{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, BertTokenizer\n",
    "import torch.optim as optim\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "from dataset.create_dataset import PileGenerator, StoriesGenerator, create_test_dataset\n",
    "from layers.model import Transformer, AutoregressiveWrapper\n",
    "from test_model.test_model import TestModel\n",
    "\n",
    "import wandb\n",
    "import yadisk\n",
    "from huggingface_hub import login\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "login('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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#!git clone https://github.com/konductor000/GenerativePretrainedTransformer\\n!pip install -r requirements.txt\\n!pip install rouge_score\\n!MAX_JOBS=4 pip install flash-attn --no-build-isolation\\n!pip install datasets\\n!pip install transformers\\n!pip install evaluate\\n!pip install spacy\\n!pip install yadisk\\n!pip install wandb\\n!pip install huggingface_hub\\n!python -m spacy download en_core_web_md\\n#8a49fefdd8a82ca9ba659a874b09adf8c5995778\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#!git clone https://github.com/konductor000/GenerativePretrainedTransformer\n",
    "!pip install -r requirements.txt\n",
    "!pip install rouge_score\n",
    "!MAX_JOBS=4 pip install flash-attn --no-build-isolation\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install evaluate\n",
    "!pip install spacy\n",
    "!pip install yadisk\n",
    "!pip install wandb\n",
    "!pip install huggingface_hub\n",
    "!python -m spacy download en_core_web_md\n",
    "#8a49fefdd8a82ca9ba659a874b09adf8c5995778\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    \"architecture\": \"Transformer\", # Wandb only\n",
    "    \"dataset\": \"pile\", #\"wikitext-103-raw-v1\", # Wandb only\n",
    "    \"batch_size\": 30,\n",
    "    \"embedding_size\": 1024,\n",
    "    \"max_sequence_length\": 512,\n",
    "    \"number_of_layers\": 24,\n",
    "    \"number_of_heads\": 16,\n",
    "    \"extention_factor\": 4,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    \"test_size\": 1024,\n",
    "    \"start_book\": 3000*1000,\n",
    "    \"test_every\": 1024,\n",
    "    \"log_traing_metrics_every\": 64,\n",
    "    'flash_atten': True,\n",
    "    'num_train_points': 2500,\n",
    "    \"save_every\": 180,\n",
    "    \"use_mixed_precision\": True,\n",
    "    \"model_path\": \"save_models/unique-water-415/model_2\",\n",
    "    \n",
    "}\n",
    "CONFIG['lr'] = 0.001 / \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def log_metrics(test_name, test_dataloader, pipeline, model_tester, CONFIG):\n",
    "    pipeline.eval()\n",
    "    if test_name in ['stories', 'pile']:\n",
    "        metrics = model_tester.test_model(pipeline, test_dataloader)\n",
    "        test_loss = metrics['loss']\n",
    "        bleu = metrics['bleu']\n",
    "        rouge1 = metrics['rouge1']\n",
    "        rouge2 = metrics['rouge2']\n",
    "        rougeL = metrics['rougeL']\n",
    "\n",
    "        wandb.log({\n",
    "            \"test_loss_\" + test_name: test_loss,\n",
    "            \"bleu_\" + test_name: bleu,\n",
    "            \"rouge1_\" + test_name: rouge1,\n",
    "            \"rouge2_\" + test_name: rouge2,\n",
    "            \"rougeL_\" + test_name: rougeL,\n",
    "        }) \n",
    "    else:\n",
    "        simmilarity_accuracy, simmilarity_score = model_tester.test_simmilarity(pipeline,\n",
    "                                                            test_dataloader, tokenizer)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"simmilarity_accuracy\": simmilarity_accuracy,\n",
    "            \"simmilarity_score\": simmilarity_score,\n",
    "        })\n",
    "\n",
    "    pipeline.train()\n",
    "        \n",
    "\n",
    "def train(CONFIG, pipeline, model, optimizer, loss_function, model_tester, wandb,\n",
    "          wandb_run, scaler, y, dataloaders):\n",
    "    pile_generator, stories_generator, pile_test_dataloader, \\\n",
    "       stories_test_dataloader, cloze_dataset = dataloaders\n",
    "\n",
    "    train_time = 0\n",
    "    total_train_time = time()\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    batch_num = 0\n",
    "    train_losses = []\n",
    "    tests = ['pile', 'stories', 'cloze']\n",
    "    test_dataloaders = [pile_test_dataloader, stories_test_dataloader, cloze_dataset]\n",
    "\n",
    "    for i in tqdm(range(10000), desc=\"Training Progress\"):\n",
    "        try:\n",
    "            train_dataloader = pile_generator.next_loader(1000)\n",
    "        except:\n",
    "            break\n",
    "        if time() - total_train_time > CONFIG['save_every'] * 60:\n",
    "            total_train_time = time()\n",
    "            save_model(model, wandb_run, y)\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            train_start = time()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                model_output, target = pipeline(input_ids, attention_mask)\n",
    "                loss = loss_function(model_output.transpose(1, 2), target)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            train_time += time() - train_start\n",
    "            batch_num += 1\n",
    "\n",
    "            if batch_num % CONFIG[\"log_traing_metrics_every\"] == 0:\n",
    "                wandb.log({\n",
    "                    \"train_loss\": sum(train_losses[-CONFIG[\"log_traing_metrics_every\"]:]) / CONFIG[\"log_traing_metrics_every\"],\n",
    "                    \"train_time\": train_time / CONFIG[\"log_traing_metrics_every\"],\n",
    "                })\n",
    "                train_time = 0\n",
    "\n",
    "            if batch_num % CONFIG[\"test_every\"] == 0:\n",
    "                test_start = time()\n",
    "                for i in range(len(tests)):\n",
    "                    log_metrics(tests[i], test_dataloaders[i], pipeline, model_tester, CONFIG)\n",
    "                wandb.log({\n",
    "                    \"test_time\": time() - test_start,\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(CONFIG, tokenizer, model_path=None):\n",
    "    number_of_tokens = tokenizer.vocab_size\n",
    "    \n",
    "    y = yadisk.YaDisk(token=\"y0_AgAAAAA7vZKNAADLWwAAAADsK4dQ-f3fKT3hSianJggcCcKC1Mfxo8s\")\n",
    "    print(y.check_token())\n",
    "    model = Transformer(\n",
    "        embedding_size=CONFIG[\"embedding_size\"],\n",
    "        number_of_tokens=number_of_tokens,\n",
    "        number_of_heads=CONFIG[\"number_of_heads\"],\n",
    "        number_of_layers=CONFIG[\"number_of_layers\"],\n",
    "        extention_factor=CONFIG[\"extention_factor\"],\n",
    "        dropout_rate=CONFIG[\"dropout_rate\"],\n",
    "        max_sequence_length=CONFIG[\"max_sequence_length\"],\n",
    "        use_flash_att=CONFIG[\"flash_atten\"],\n",
    "    ).to(device)\n",
    "    if model_path:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    pipeline = AutoregressiveWrapper(model).to(device)\n",
    "    loss_function = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
    "    model_tester = TestModel(tokenizer, model)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    return pipeline, model, optimizer, loss_function, model_tester, scaler, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, wandb_run, y):\n",
    "    if not os.path.exists('save_models'):\n",
    "        os.makedirs('save_models')\n",
    "    save_dir = f\"save_models/{wandb_run.name}\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    saved_models = [f for f in os.listdir(save_dir) if f.startswith(\"model_\")]\n",
    "    num_saved_models = len(saved_models)\n",
    "\n",
    "    model_filename = f\"model_{num_saved_models + 1}\"\n",
    "    model_path = f'{save_dir}/{model_filename}'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    try:\n",
    "        y.mkdir(save_dir)\n",
    "    except yadisk.exceptions.PathExistsError:\n",
    "        pass\n",
    "    print(model_path)\n",
    "    y.upload(model_path, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskorodumov-work\u001b[0m (\u001b[33m8667\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/GenerativePretrainedTransformer/wandb/run-20230927_111913-xeo2nuq9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Resuming run <strong><a href='https://wandb.ai/8667/transformer/runs/xeo2nuq9' target=\"_blank\">unique-water-415</a></strong> to <a href='https://wandb.ai/8667/transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/8667/transformer' target=\"_blank\">https://wandb.ai/8667/transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/8667/transformer/runs/xeo2nuq9' target=\"_blank\">https://wandb.ai/8667/transformer/runs/xeo2nuq9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_run = wandb.init(\n",
    "    project=\"transformer\",\n",
    "    tags=[\"long_training_testing\"],\n",
    "    id=\"xeo2nuq9\",\n",
    "    resume=\"allow\",\n",
    "    config=CONFIG\n",
    ")\n",
    "\n",
    "load_path = CONFIG['model_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "number of parameters = 361724228\n",
      "number of trainable parameters = 361724228\n",
      "memory allocated in GB = 1.3475277572870255\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "pile_generator = PileGenerator(batch_size=CONFIG[\"batch_size\"], max_sequence_length=CONFIG[\"max_sequence_length\"],\n",
    "                                tokenizer=tokenizer, num_workers=16, test_size=1024, num_skip=CONFIG[\"start_book\"])\n",
    "stories_generator = StoriesGenerator(batch_size=CONFIG[\"batch_size\"], max_sequence_length=CONFIG[\"max_sequence_length\"], \n",
    "                               tokenizer=tokenizer, num_workers=16, test_size=1024)\n",
    "cloze_dataset = create_test_dataset('data.csv')\n",
    "\n",
    "pipeline, model, optimizer, loss_function, model_tester, scaler, y = create_model(CONFIG, tokenizer, CONFIG[\"model_path\"])\n",
    "num_parameters, num_trainable_parameters, memory_allocated = pipeline.count_parameters() \n",
    "print('number of parameters =', num_parameters)\n",
    "print('number of trainable parameters =', num_trainable_parameters)\n",
    "print('memory allocated in GB =', memory_allocated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   3%|▎         | 330/10000 [3:00:17<72:00:46, 26.81s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_models/unique-water-415/model_3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   5%|▍         | 468/10000 [4:21:53<88:53:58, 33.58s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_tester\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m     \u001b[49m\u001b[43m[\u001b[49m\u001b[43mpile_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstories_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpile_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstories_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcloze_dataset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 65\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(CONFIG, pipeline, model, optimizer, loss_function, model_tester, wandb, wandb_run, scaler, y, dataloaders)\u001b[0m\n\u001b[1;32m     62\u001b[0m     model_output, target \u001b[38;5;241m=\u001b[39m pipeline(input_ids, attention_mask)\n\u001b[1;32m     63\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_function(model_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), target)\n\u001b[0;32m---> 65\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m scaler\u001b[38;5;241m.\u001b[39mstep(optimizer)\n\u001b[1;32m     67\u001b[0m scaler\u001b[38;5;241m.\u001b[39mupdate()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(CONFIG, pipeline, model, optimizer, loss_function, model_tester, wandb, wandb_run, scaler, y,\n",
    "     [pile_generator, stories_generator, pile_generator.test_dataloader, stories_generator.test_dataloader, cloze_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_models/unique-water-415/model_4\n"
     ]
    }
   ],
   "source": [
    "save_model(model, wandb_run, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6710450649261475\n",
      "[CLS] One day Alice and her pet dog, Max, were playing in the park. Suddenly, they heard a loud noise coming from a nearby tree. \" What was that? \" asked Alice. \" I don't know, \" said Max. \" Let's go see! \" So they ran towards the tree. When they got closer, they saw that it was a little bird with a broken wing. The bird was hurt and couldn't fly. \" Oh no! \" said Alice. \" Let's help the bird. \" So they carefully picked up the bird and took it to the vet. The vet checked the bird's wing and said, \" Don't worry, we'll take it to the vet. \" The vet took the bird to the vet. The vet checked the bird's wing and said, \" Let's take it to the vet. \" The vet took the bird to the vet. The vet checked the bird '\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"\n",
    "One day Alice and her pet\n",
    "\"\"\"\n",
    "num_predicted_tokens = 200\n",
    "k = 3\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-large-cased')\n",
    "\n",
    "start = time()\n",
    "with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "    answer = pipeline.predict_next(input_text, tokenizer, num_predicted_tokens, k)\n",
    "print(time() - start)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def predict_next(pipeline, input_text, num_predicted_tokens, tokenizer, k):\n",
    "    input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    input_tokens = input_tokens[:, :-1].to(device)\n",
    "    \n",
    "    for i in range(num_predicted_tokens):\n",
    "        mask = torch.ones_like(input_tokens)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probabilities = pipeline.next_token_probabilities(input_tokens, mask)\n",
    "        \n",
    "        answer = probabilities.argsort(dim=-1)[:, -randint(1, k)].unsqueeze(0)\n",
    "        input_tokens = torch.cat((input_tokens, answer), dim=1)\n",
    "        \n",
    "    return tokenizer.decode(input_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24002885818481445\n",
      "[CLS] Suddenly the boy saw a dog and ran to him with a pistol. He was very excited to get it, but the dog didn'chew it. The dog was so scared he started to cry. He tried his arm around the pistol, trying but the dog was too quick and\n"
     ]
    }
   ],
   "source": [
    "input_text = \"\"\"\n",
    "Suddenly the boy saw a dog and\n",
    "\"\"\"\n",
    "num_predicted_tokens = 50\n",
    "k = 2\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-large-cased')\n",
    "\n",
    "start = time()\n",
    "with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "    answer = predict_next(pipeline, input_text, num_predicted_tokens, tokenizer, k)\n",
    "print(time() - start)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4101223945617676\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "train_dataloader = stories_generator.next_loader(1000)      \n",
    "print(time() - start)\n",
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6824512481689453\n"
     ]
    }
   ],
   "source": [
    "loader_time = time()\n",
    "for batch in train_dataloader:\n",
    "    pass\n",
    "print(time() - loader_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 5.29 GiB (GPU 0; 23.69 GiB total capacity; 14.87 GiB already allocated; 4.08 GiB free; 19.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m grad_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m     26\u001b[0m time1 \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m---> 27\u001b[0m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m time2 \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m     29\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 5.29 GiB (GPU 0; 23.69 GiB total capacity; 14.87 GiB already allocated; 4.08 GiB free; 19.24 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "data_prep_timings = []\n",
    "model_train_timings = []\n",
    "gradient_timings = []\n",
    "times1 = []\n",
    "times2 = []\n",
    "times3 = []\n",
    "times4 = []\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    data_prep_time = time()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "    data_prep_timings.append(time() - data_prep_time)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    model_time = time()\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        model_output, target = pipeline(input_ids, attention_mask)\n",
    "        loss = loss_function(model_output.transpose(1, 2), target)\n",
    "\n",
    "    model_train_timings.append(time() - model_time)\n",
    "\n",
    "    grad_time = time()\n",
    "    time1 = time()\n",
    "    scaler.scale(loss).backward()\n",
    "    time2 = time()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    time3 = time()\n",
    "    scaler.step(optimizer)\n",
    "    time4 = time()\n",
    "    scaler.update()\n",
    "    times1.append(time2-time1)\n",
    "    times2.append(time3-time2)\n",
    "    times3.append(time4-time3)\n",
    "    times4.append(time()-time4)\n",
    "    gradient_timings.append(time() - grad_time)\n",
    "\n",
    "print('number of batches -', len(train_dataloader))\n",
    "print('data_prep_timings -', sum(data_prep_timings))\n",
    "print('model_train_timings -', sum(model_train_timings))\n",
    "print('gradient_timings -', sum(gradient_timings))\n",
    "print('backward -', sum(times1))\n",
    "print('clipping -', sum(times2))\n",
    "print('step -', sum(times3))\n",
    "print('updating -', sum(times4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-09-18 18:28:48 17096:17096 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-09-18 18:29:31 17096:17096 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-09-18 18:29:32 17096:17096 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        start_time = time()\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "            train_start = time()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                model_output, target = pipeline(input_ids, attention_mask)\n",
    "                loss = loss_function(model_output.transpose(1, 2), target)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference        48.52%       37.132s        54.17%       41.454s       41.454s             1  \n",
      "                                  cudaStreamSynchronize        42.85%       32.795s        42.85%       32.795s      48.228ms           680  \n",
      "autograd::engine::evaluate_function: EmbeddingBackwa...         0.00%       1.433ms        42.00%       32.144s     236.354ms           136  \n",
      "                                     EmbeddingBackward0        -0.31%  -238186.000us        42.00%       32.143s     236.344ms           136  \n",
      "                               aten::embedding_backward         0.31%     239.195ms        42.00%       32.142s     236.340ms           136  \n",
      "                         aten::embedding_dense_backward         0.03%      19.453ms        42.00%       32.142s     236.336ms           136  \n",
      "                                               aten::to         0.20%     155.069ms         2.42%        1.856s      31.833us         58292  \n",
      "                                         aten::_to_copy         0.40%     304.029ms         2.32%        1.776s      30.898us         57476  \n",
      "                                           aten::linear         0.16%     118.870ms         2.00%        1.530s     114.802us         13328  \n",
      "                                            aten::copy_         0.36%     276.520ms         1.64%        1.254s      21.558us         58156  \n",
      "                               Optimizer.step#Adam.step         0.50%     380.564ms         1.61%        1.228s       9.306ms           132  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.14%     103.364ms         1.34%        1.026s     153.950us          6664  \n",
      "                                         AddmmBackward0         0.11%      83.165ms         0.91%     695.585ms     104.380us          6664  \n",
      "                                       cudaLaunchKernel         0.85%     653.615ms         0.85%     653.615ms       6.050us        108032  \n",
      "autograd::engine::evaluate_function: ToCopyBackward0...         0.06%      47.304ms         0.77%     585.602ms      34.725us         16864  \n",
      "                                               aten::mm         0.50%     380.925ms         0.65%     498.341ms      37.391us         13328  \n",
      "                                        ToCopyBackward0         0.09%      65.163ms         0.64%     487.507ms      28.908us         16864  \n",
      "                                    aten::empty_strided         0.63%     484.431ms         0.63%     484.431ms       4.750us        101980  \n",
      "                                            aten::addmm         0.37%     281.082ms         0.48%     370.806ms      55.643us          6664  \n",
      "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...         0.43%     330.520ms         0.44%     333.055ms       2.431ms           137  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 76.530s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: london is the capital of\n",
      "output:  his family. He loves his family very much. One day, his mom and dad took him to the park. It was a sunny day and the sun is shining brightly. As they were walking,\n",
      "----------------------------------------\n",
      "input: The Eiffel Tower is located in\n",
      "output:  the living room. She likes to play with her toys and her friends. One day, Eiff went to the park with her mom. At the park, Eiff saw a big slide. She\n",
      "----------------------------------------\n",
      "input: The Amazon River flows through several countries including\n",
      "output:  a lake. The lake was full of colourful fish, and the fish were very happy. One day, a big storm came to the lake. All the fish in the lake were very scared.\n",
      "----------------------------------------\n",
      "input: The Amazon River flows through several countries, isn't\n",
      "output:  very strong. It was so strong that no one could touch it. One day, a little 3 year old child saw the ant and wanted to play with it. The child asked the ant, \"\n",
      "----------------------------------------\n",
      "input: The Declaration of Independence was signed in\n",
      "output:  the sky. He was so excited to see the world. He had never seen anything like it before. He had never seen anything like it before. He had never seen anything like it before. He\n",
      "----------------------------------------\n",
      "input: The Sahara Desert is located in\n",
      "output:  the garden. It is a beautiful day and the sun is shining and the birds were singing. Suddenly, a little bird flew into the garden. The little bird was curious and wanted to see what was\n",
      "----------------------------------------\n",
      "input: The big building where you learn new things and meet friends is a\n",
      "output:  lot of people. One day, the people in the building decided to have a meeting. They wanted to see what was inside the building. The meeting was full of interesting things to see and learn.\n",
      "----------------------------------------\n",
      "input: When you want to travel around the city, you can take a colorful car called a\n",
      "output: eroplane. One day, the aeroplane arrived at the airport. It was big and blue and had lots of windows. The aeroplane was so excited that it zoomed up and down\n",
      "----------------------------------------\n",
      "input: Dogs are known for their loyalty and\n",
      "output:  determination. They like to go to the park and play with their friends. One day, they go to the park with their moms. They see a big slide, a swing, and a sand\n",
      "----------------------------------------\n",
      "input: The sun sets in the\n",
      "output:  sky and it was a beautiful day. The sun was shining and the birds were singing in the trees. The birds were singing and the trees were swaying in the wind. Suddenly, a loud noise came\n",
      "----------------------------------------\n",
      "input: Owls are nocturnal birds, which means they are most active\n",
      "output: . One day, Owls decided to go for a walk in the park. As he was walking, he heard a loud noise. He stopped and looked around. He saw a group of birds\n",
      "----------------------------------------\n",
      "input: Abraham Lincoln was the President of the United States during the\n",
      "output:  day. He was so excited that he couldn't wait to see what it would be like. One day, he decided to go for a walk in the woods. As he was walking, he\n",
      "----------------------------------------\n",
      "input: Mount Everest is the highest peak in\n",
      "output:  the park. He likes to run and play with his friends. One day, he sees a big tree with lots of leaves. He wants to climb the tree and see what is on the other side\n",
      "----------------------------------------\n",
      "input: Music has the power to evoke strong\n",
      "output: . Everyone in the village was scared of it. One day, a brave little boy named Jack decided to try something new. He took a deep breath and took a deep breath. He took a deep\n",
      "----------------------------------------\n",
      "input: The Great Barrier Reef is a famous natural wonder located in\n",
      "output:  the sea. Every day, the sun shined brightly in the sky. One day, as the sun was setting, the sky lit up with a beautiful sight. The sky was filled with beautiful colors\n",
      "----------------------------------------\n",
      "input: The Earth orbits around the\n",
      "output:  beach. It was a beautiful day and the sun was shining brightly. The sun was shining and the birds were singing in the sky. Suddenly, a big wave came and swept the Earth away. The\n",
      "----------------------------------------\n",
      "input: Honey is produced by\n",
      "output:  the sea. She loves to play in the sand and look for shells. One day, she finds a big shell in the sand. It was shiny and smooth. Honey was very happy. She wanted\n",
      "----------------------------------------\n",
      "input: When you're feeling unwell, it's important to get plenty of\n",
      "output:  rest. One day, it was raining outside and the sun was shining brightly. Suddenly, a loud noise came from the sky. It was so loud that it made the ground shake. It was\n",
      "----------------------------------------\n",
      "input: A caterpillar transforms into a\n",
      "output:  garden. He wanted to explore the garden, but his mom said no. She said the caterpillar was too big and too small. The caterpillar was sad, but he still wanted to\n",
      "----------------------------------------\n",
      "input: The moon is Earth's natural\n",
      "output:  in the middle of the Earth. It is a beautiful place, with lots of trees and flowers. One day, a little boy named Tim went to the beach with his mom and dad. He saw\n",
      "----------------------------------------\n",
      "input: In cold weather, water can freeze and become\n",
      "output:  cold. One day, a little boy named Tim went to the park with his mom. They saw a big tree with a lot of leaves. Tim wanted to climb the tree, but his mom said\n",
      "----------------------------------------\n",
      "input: Many birds migrate to warmer areas during the\n",
      "output:  sunny day. One day, a small bird flew up to the sky and perched on a branch. The bird was curious and wanted to know what the bird was doing. So, the bird flew down\n",
      "----------------------------------------\n",
      "input: The Statue of Liberty holds a\n",
      "output:  beautiful flower in the garden. Every day, the flower bloomed in the sunshine. One day, the flower bloomed in the garden. It was so beautiful! The flower bloomed\n",
      "----------------------------------------\n",
      "input: The capital city of Japan is\n",
      "output:  far away. Everyone in the city were scared of it. One day, a little boy named Tim went to the city. He was only three years old, but he was brave. He wanted to\n",
      "----------------------------------------\n",
      "input: The sun rises in the\n",
      "output:  sky and it was a beautiful day. The sun was shining and the birds were singing in the trees. Suddenly, a loud noise came from the sky. It was loud and scary. Everyone was scared\n",
      "----------------------------------------\n",
      "input: Flowers need sunlight and water to\n",
      "output:  stay in the garden. Every day, the sun shone brightly in the sky and made the garden look beautiful. One day, the sun was shining brightly and the sky was blue. Suddenly, it started\n",
      "----------------------------------------\n",
      "input: Fish live in\n",
      "output:  a small village. He was very happy and loved to play with his friends. One day, he decided to go for a walk in the woods. As he was walking, he heard a loud noise\n",
      "----------------------------------------\n",
      "input: People wear coats in\n",
      "output:  the park every day. One day, a little boy named Tim went to the park with his mom. He saw a big tree and wanted to climb it. His mom said, \" Be careful,\n",
      "----------------------------------------\n",
      "input: The sky is usually\n",
      "output:  blue. It is a sunny day, and the sun is shining. A little boy named Tim went to the park with his mom. They saw a big slide. Tim wanted to go on the slide\n",
      "----------------------------------------\n",
      "input: Water is essential for\n",
      "output:  three years old. It is a sunny day and the sky is blue. It is raining and the sky is blue. Suddenly, it starts to rain. The raindrops drops on the ground\n",
      "----------------------------------------\n",
      "input: The moon shines at\n",
      "output:  the bottom of the sky. A little girl named Lucy was playing in the garden. She saw the moon and wanted to touch it. She asked her mom, \" Can I touch the moon? \"\n",
      "----------------------------------------\n",
      "input: Apples can be green, or\n",
      "output:  blue. She likes to play with her friends in the park. One day, she sees a big tree. She wants to climb the tree. She thinks it will be fun. She runs to the\n",
      "----------------------------------------\n",
      "input: Milk comes from\n",
      "output:  the cupboard in the living room. The cupboard was big and had a lot of things on it. One day, the cupboard started to shrink. It became smaller and smaller until it was\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "k=4\n",
    "for input_text in data['Sentence']:\n",
    "    num_predicted_tokens = 40\n",
    "    \n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        answer = pipeline.predict_next(input_text, tokenizer, num_predicted_tokens, k)\n",
    "\n",
    "    print('input:', input_text)\n",
    "    print('output:', answer[len(input_text)+6:])\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y.download(\"save_models/ethereal-planet-258/model_1\", \"model_1\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "8defd53a528e9245923bfdc9d5a38f5c3ca09cbc6e92fa1b95a37aeffbb04cd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
