{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, BertTokenizer\n",
    "import torch.optim as optim\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import numpy as np\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from time import time\n",
    "import pandas as pd\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from dataset.create_dataset import PileGenerator, StoriesGenerator, create_test_dataset\n",
    "from layers.model import Transformer, AutoregressiveWrapper\n",
    "from test_model.test_model import TestModel\n",
    "\n",
    "import wandb\n",
    "import yadisk\n",
    "from huggingface_hub import login\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "login('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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#!git clone https://github.com/konductor000/GenerativePretrainedTransformer\\n!pip install -r requirements.txt\\n!pip install rouge_score\\n!MAX_JOBS=4 pip install flash-attn --no-build-isolation\\n!pip install datasets\\n!pip install transformers\\n!pip install evaluate\\n!pip install spacy\\n!pip install yadisk\\n!pip install wandb\\n!python -m spacy download en_core_web_md\\n#8a49fefdd8a82ca9ba659a874b09adf8c5995778\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#!git clone https://github.com/konductor000/GenerativePretrainedTransformer\n",
    "!pip install -r requirements.txt\n",
    "!pip install rouge_score\n",
    "!MAX_JOBS=4 pip install flash-attn --no-build-isolation\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install evaluate\n",
    "!pip install spacy\n",
    "!pip install yadisk\n",
    "!pip install wandb\n",
    "!pip install huggingface_hub\n",
    "!python -m spacy download en_core_web_md\n",
    "#8a49fefdd8a82ca9ba659a874b09adf8c5995778\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    \"architecture\": \"Transformer\", # Wandb only\n",
    "    \"dataset\": \"books\", #\"wikitext-103-raw-v1\", # Wandb only\n",
    "    \"batch_size\": 64,\n",
    "    \"embedding_size\": 256,\n",
    "    \"max_sequence_length\": 256,\n",
    "    \"number_of_layers\": 12,\n",
    "    \"number_of_heads\": 8,\n",
    "    \"extention_factor\": 4,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    'test_size': 1024,\n",
    "    'start_book': 0,\n",
    "    \"test_every\": 2**15,\n",
    "    \"log_traing_metrics_every\": 2**8,\n",
    "    'flash_atten': True,\n",
    "    'num_train_points': 2500,\n",
    "    'save_every': 30,\n",
    "    'use_mixed_precision': True,\n",
    "    'model_path': None, # \"savepoints/dulcet-serenity-218\",\n",
    "    \n",
    "}\n",
    "CONFIG[\"lr\"] = 0.001 / np.sqrt(CONFIG[\"batch_size\"])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def log_metrics(test_name, test_dataloader, pipeline, model_tester, CONFIG):\n",
    "    pipeline.eval()\n",
    "    if test_name in ['stories', 'pile']:\n",
    "        metrics = model_tester.test_model(pipeline, test_dataloader)\n",
    "        test_loss = metrics['loss']\n",
    "        bleu = metrics['bleu']\n",
    "        rouge1 = metrics['rouge1']\n",
    "        rouge2 = metrics['rouge2']\n",
    "        rougeL = metrics['rougeL']\n",
    "\n",
    "        wandb.log({\n",
    "            \"test_loss_\" + test_name: test_loss,\n",
    "            \"bleu_\" + test_name: bleu,\n",
    "            \"rouge1_\" + test_name: rouge1,\n",
    "            \"rouge2_\" + test_name: rouge2,\n",
    "            \"rougeL_\" + test_name: rougeL,\n",
    "        }) \n",
    "    else:\n",
    "        simmilarity_accuracy, simmilarity_score = model_tester.test_simmilarity(pipeline,\n",
    "                                                            test_dataloader, tokenizer)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"simmilarity_accuracy\": simmilarity_accuracy,\n",
    "            \"simmilarity_score\": simmilarity_score,\n",
    "        })\n",
    "\n",
    "    pipeline.train()\n",
    "        \n",
    "\n",
    "def train(CONFIG, pipeline, model, optimizer, loss_function, model_tester, wandb,\n",
    "          wandb_run, scaler, y, dataloaders):\n",
    "    stories_generator, pile_test_dataloader, stories_test_dataloader, cloze_dataset = dataloaders\n",
    "\n",
    "    train_time = 0\n",
    "    total_train_time = time()\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    batch_num = 0\n",
    "    train_losses = []\n",
    "    tests = ['stories', 'pile', 'cloze']\n",
    "    test_dataloaders = [pile_test_dataloader, stories_test_dataloader, cloze_dataset]\n",
    "    \n",
    "    for i in tqdm(range(CONFIG['num_train_books']), desc=\"Training Progress\"):\n",
    "        train_dataloader = stories_generator.next_loader(100)\n",
    "        if time() - total_train_time > CONFIG['save_every'] * 60:\n",
    "            total_train_time = time()\n",
    "            save_model(model, wandb_run, y)\n",
    "            \n",
    "        for batch in train_dataloader:\n",
    "            train_start = time()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                model_output, target = pipeline(input_ids, attention_mask)\n",
    "                loss = loss_function(model_output.transpose(1, 2), target)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "            train_time += time() - train_start\n",
    "            batch_num += 1\n",
    "    \n",
    "            if batch_num % CONFIG[\"log_traing_metrics_every\"] == 0:\n",
    "                wandb.log({\n",
    "                    \"train_loss\": sum(train_losses[-CONFIG[\"log_traing_metrics_every\"]:]) / CONFIG[\"log_traing_metrics_every\"],\n",
    "                    \"train_time\": train_time / CONFIG[\"log_traing_metrics_every\"],\n",
    "                })\n",
    "                train_time = 0\n",
    "            \n",
    "            if batch_num % CONFIG[\"test_every\"] == 0:\n",
    "                test_start = time()\n",
    "                for i in range(len(tests)):\n",
    "                    log_metrics(tests[i], test_dataloaders[i], pipeline, model_tester, CONFIG)\n",
    "                wandb.log({\n",
    "                    \"test_time\": time() - test_start,\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(CONFIG, tokenizer, model_path=None):\n",
    "    number_of_tokens = tokenizer.vocab_size\n",
    "    \n",
    "    y = yadisk.YaDisk(token=\"y0_AgAAAAA7vZKNAADLWwAAAADsK4dQ-f3fKT3hSianJggcCcKC1Mfxo8s\")\n",
    "    print(y.check_token())\n",
    "    model = Transformer(\n",
    "        embedding_size=CONFIG[\"embedding_size\"],\n",
    "        number_of_tokens=number_of_tokens,\n",
    "        number_of_heads=CONFIG[\"number_of_heads\"],\n",
    "        number_of_layers=CONFIG[\"number_of_layers\"],\n",
    "        extention_factor=CONFIG[\"extention_factor\"],\n",
    "        dropout_rate=CONFIG[\"dropout_rate\"],\n",
    "        max_sequence_length=CONFIG[\"max_sequence_length\"],\n",
    "        use_flash_att=CONFIG[\"flash_atten\"],\n",
    "    ).to(device)\n",
    "    if model_path:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "    pipeline = AutoregressiveWrapper(model).to(device)\n",
    "    loss_function = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
    "    model_tester = TestModel(tokenizer, model)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    return pipeline, model, optimizer, loss_function, model_tester, scaler, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, wandb_run, y):\n",
    "    if not os.path.exists('save_models'):\n",
    "        os.makedirs('save_models')\n",
    "    save_dir = f\"save_models/{wandb_run.name}\"\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    saved_models = [f for f in os.listdir(save_dir) if f.startswith(\"model_\")]\n",
    "    num_saved_models = len(saved_models)\n",
    "\n",
    "    model_filename = f\"model_{num_saved_models + 1}\"\n",
    "    model_path = f'{save_dir}/{model_filename}'\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    \n",
    "    try:\n",
    "        y.mkdir(save_dir)\n",
    "    except yadisk.exceptions.PathExistsError:\n",
    "        pass\n",
    "    print(model_path)\n",
    "    y.upload(model_path, model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskorodumov-work\u001b[0m (\u001b[33m8667\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/GenerativePretrainedTransformer/wandb/run-20230918_181958-gthy7uuk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/8667/transformer/runs/gthy7uuk' target=\"_blank\">comic-lion-361</a></strong> to <a href='https://wandb.ai/8667/transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/8667/transformer' target=\"_blank\">https://wandb.ai/8667/transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/8667/transformer/runs/gthy7uuk' target=\"_blank\">https://wandb.ai/8667/transformer/runs/gthy7uuk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_run = wandb.init(\n",
    "    project=\"transformer\",\n",
    "    tags=[\"long_training_testing\"],\n",
    "    #id=\"sblota90\",\n",
    "    resume=\"allow\",\n",
    "    config=CONFIG\n",
    ")\n",
    "\n",
    "load_path = CONFIG['model_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1d6911712540d79fad8711e9f825ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "number of parameters = 129622852\n",
      "number of trainable parameters = 129622852\n",
      "memory allocated in GB = 0.4828827530145645\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "pile_generator = PileGenerator(batch_size=CONFIG[\"batch_size\"], max_sequence_length=CONFIG[\"max_sequence_length\"],\n",
    "                                tokenizer=tokenizer, num_workers=16, test_size=4096, num_skip=0)\n",
    "stories_generator = StoriesGenerator(batch_size=CONFIG[\"batch_size\"], max_sequence_length=CONFIG[\"max_sequence_length\"], \n",
    "                               tokenizer=tokenizer, num_workers=16)\n",
    "cloze_dataset = create_test_dataset('data.csv')\n",
    "\n",
    "pipeline, model, optimizer, loss_function, model_tester, scaler, y = create_model(CONFIG, tokenizer, CONFIG[\"model_path\"])\n",
    "num_parameters, num_trainable_parameters, memory_allocated = pipeline.count_parameters() \n",
    "print('number of parameters =', num_parameters)\n",
    "print('number of trainable parameters =', num_trainable_parameters)\n",
    "print('memory allocated in GB =', memory_allocated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(CONFIG, pipeline, model, optimizer, loss_function, model_tester, wandb, wandb_run, scaler, y,\n",
    "     [stories_generator, pile_generator.test_dataloader, stories_generator.test_dataloader, cloze_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, wandb_run, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Mount Everest is the highest peak in\n",
    "\"\"\"\n",
    "num_predicted_tokens = 20\n",
    "k = 3\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "\n",
    "start = time()\n",
    "with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "    answer = pipeline.predict_next(input_text, tokenizer, num_predicted_tokens, k)\n",
    "print(time() - start)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def predict_next(pipeline, input_text, num_predicted_tokens, tokenizer):\n",
    "    input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    input_tokens = input_tokens[:, :-1].to(device)\n",
    "    \n",
    "    for i in range(num_predicted_tokens):\n",
    "        mask = torch.ones_like(input_tokens)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probabilities = pipeline.next_token_probabilities(input_tokens, mask)\n",
    "        \n",
    "        answer = probabilities.argsort(dim=-1)[:, -randint(1, 3)].unsqueeze(0)\n",
    "        input_tokens = torch.cat((input_tokens, answer), dim=1)\n",
    "        \n",
    "    return tokenizer.decode(input_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.44458270072937\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m book_generator\u001b[38;5;241m.\u001b[39mnext_book(\u001b[38;5;241m10\u001b[39m)      \n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(time() \u001b[38;5;241m-\u001b[39m start)\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mdataloader\u001b[49m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "train_dataloader = book_generator.next_book(10)      \n",
    "print(time() - start)\n",
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8038690090179443\n"
     ]
    }
   ],
   "source": [
    "loader_time = time()\n",
    "for batch in train_dataloader:\n",
    "    pass\n",
    "print(time() - loader_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches - 136\n",
      "data_prep_timings - 1.1094281673431396\n",
      "model_train_timings - 1.5837981700897217\n",
      "gradient_timings - 37.77984666824341\n",
      "backward - 36.2930953502655\n",
      "clipping - 0.39544034004211426\n",
      "step - 1.084547519683838\n",
      "updating - 0.006574153900146484\n"
     ]
    }
   ],
   "source": [
    "data_prep_timings = []\n",
    "model_train_timings = []\n",
    "gradient_timings = []\n",
    "times1 = []\n",
    "times2 = []\n",
    "times3 = []\n",
    "times4 = []\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    data_prep_time = time()\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "    data_prep_timings.append(time() - data_prep_time)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    model_time = time()\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        model_output, target = pipeline(input_ids, attention_mask)\n",
    "        loss = loss_function(model_output.transpose(1, 2), target)\n",
    "\n",
    "    model_train_timings.append(time() - model_time)\n",
    "\n",
    "    grad_time = time()\n",
    "    time1 = time()\n",
    "    scaler.scale(loss).backward()\n",
    "    time2 = time()\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "    time3 = time()\n",
    "    scaler.step(optimizer)\n",
    "    time4 = time()\n",
    "    scaler.update()\n",
    "    times1.append(time2-time1)\n",
    "    times2.append(time3-time2)\n",
    "    times3.append(time4-time3)\n",
    "    times4.append(time()-time4)\n",
    "    gradient_timings.append(time() - grad_time)\n",
    "\n",
    "print('number of batches -', len(train_dataloader))\n",
    "print('data_prep_timings -', sum(data_prep_timings))\n",
    "print('model_train_timings -', sum(model_train_timings))\n",
    "print('gradient_timings -', sum(gradient_timings))\n",
    "print('backward -', sum(times1))\n",
    "print('clipping -', sum(times2))\n",
    "print('step -', sum(times3))\n",
    "print('updating -', sum(times4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2023-09-18 18:28:48 17096:17096 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2023-09-18 18:29:31 17096:17096 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2023-09-18 18:29:32 17096:17096 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU], record_shapes=True) as prof:\n",
    "    with record_function(\"model_inference\"):\n",
    "        start_time = time()\n",
    "        \n",
    "        for batch in train_dataloader:\n",
    "            train_start = time()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "                model_output, target = pipeline(input_ids, attention_mask)\n",
    "                loss = loss_function(model_output.transpose(1, 2), target)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference        48.52%       37.132s        54.17%       41.454s       41.454s             1  \n",
      "                                  cudaStreamSynchronize        42.85%       32.795s        42.85%       32.795s      48.228ms           680  \n",
      "autograd::engine::evaluate_function: EmbeddingBackwa...         0.00%       1.433ms        42.00%       32.144s     236.354ms           136  \n",
      "                                     EmbeddingBackward0        -0.31%  -238186.000us        42.00%       32.143s     236.344ms           136  \n",
      "                               aten::embedding_backward         0.31%     239.195ms        42.00%       32.142s     236.340ms           136  \n",
      "                         aten::embedding_dense_backward         0.03%      19.453ms        42.00%       32.142s     236.336ms           136  \n",
      "                                               aten::to         0.20%     155.069ms         2.42%        1.856s      31.833us         58292  \n",
      "                                         aten::_to_copy         0.40%     304.029ms         2.32%        1.776s      30.898us         57476  \n",
      "                                           aten::linear         0.16%     118.870ms         2.00%        1.530s     114.802us         13328  \n",
      "                                            aten::copy_         0.36%     276.520ms         1.64%        1.254s      21.558us         58156  \n",
      "                               Optimizer.step#Adam.step         0.50%     380.564ms         1.61%        1.228s       9.306ms           132  \n",
      "    autograd::engine::evaluate_function: AddmmBackward0         0.14%     103.364ms         1.34%        1.026s     153.950us          6664  \n",
      "                                         AddmmBackward0         0.11%      83.165ms         0.91%     695.585ms     104.380us          6664  \n",
      "                                       cudaLaunchKernel         0.85%     653.615ms         0.85%     653.615ms       6.050us        108032  \n",
      "autograd::engine::evaluate_function: ToCopyBackward0...         0.06%      47.304ms         0.77%     585.602ms      34.725us         16864  \n",
      "                                               aten::mm         0.50%     380.925ms         0.65%     498.341ms      37.391us         13328  \n",
      "                                        ToCopyBackward0         0.09%      65.163ms         0.64%     487.507ms      28.908us         16864  \n",
      "                                    aten::empty_strided         0.63%     484.431ms         0.63%     484.431ms       4.750us        101980  \n",
      "                                            aten::addmm         0.37%     281.082ms         0.48%     370.806ms      55.643us          6664  \n",
      "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...         0.43%     330.520ms         0.44%     333.055ms       2.431ms           137  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 76.530s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-------------------------------------------------------  ------------  ------------  ------------  \n",
    "                                                   Name     CPU total  CPU time avg    # of Calls  \n",
    "-------------------------------------------------------  ------------  ------------  ------------  \n",
    "                                        model_inference       41.454s       41.454s             1  \n",
    "                                  cudaStreamSynchronize       32.795s      48.228ms           680  \n",
    "autograd::engine::evaluate_function: EmbeddingBackwa...       32.144s     236.354ms           136  \n",
    "                                     EmbeddingBackward0       32.143s     236.344ms           136  \n",
    "                               aten::embedding_backward       32.142s     236.340ms           136  \n",
    "                         aten::embedding_dense_backward       32.142s     236.336ms           136  \n",
    "                                               aten::to        1.856s      31.833us         58292  \n",
    "                                         aten::_to_copy        1.776s      30.898us         57476  \n",
    "                                           aten::linear        1.530s     114.802us         13328  \n",
    "                                            aten::copy_        1.254s      21.558us         58156  \n",
    "                               Optimizer.step#Adam.step        1.228s       9.306ms           132  \n",
    "    autograd::engine::evaluate_function: AddmmBackward0        1.026s     153.950us          6664  \n",
    "                                         AddmmBackward0     695.585ms     104.380us          6664  \n",
    "                                       cudaLaunchKernel     653.615ms       6.050us        108032  \n",
    "autograd::engine::evaluate_function: ToCopyBackward0...     585.602ms      34.725us         16864  \n",
    "                                               aten::mm     498.341ms      37.391us         13328  \n",
    "                                        ToCopyBackward0     487.507ms      28.908us         16864  \n",
    "                                    aten::empty_strided     484.431ms       4.750us        101980  \n",
    "                                            aten::addmm     370.806ms      55.643us          6664  \n",
    "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...     333.055ms       2.431ms           137  \n",
    "-------------------------------------------------------  ------------  ------------  ------------  \n",
    "Self CPU time total: 76.530s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "------------------------------------------------------- ------------ ----------- \n",
    "                                                   Name    CPU total  CUDA total  \n",
    "------------------------------------------------------- ------------ ----------- \n",
    "                                        model_inference       1.621s   252.116ms \n",
    "enumerate(DataLoader)#_MultiProcessingDataLoaderIter...    231.692ms     0.000us \n",
    "                                               aten::to    216.764ms    12.108ms \n",
    "                                         aten::_to_copy    213.804ms    12.154ms \n",
    "                                            aten::copy_    206.785ms    12.175ms \n",
    "                                  cudaStreamSynchronize    180.799ms     0.000us \n",
    "                                           aten::linear     52.767ms   362.147ms \n",
    "                                        cudaMemcpyAsync     18.501ms     0.000us \n",
    "                                            aten::addmm     13.633ms   179.764ms \n",
    "                                       cudaLaunchKernel      9.117ms     0.000us \n",
    "                                 FlashAttnQKVPackedFunc      6.843ms    21.403ms \n",
    "                                       aten::layer_norm      5.672ms    12.687ms \n",
    "                                aten::native_layer_norm      5.140ms    12.687ms \n",
    "                                            aten::empty      5.054ms     0.000us \n",
    "                                    aten::empty_strided      4.312ms     0.000us \n",
    "                                              aten::add      2.524ms    12.295ms \n",
    "                                          aten::dropout      2.313ms     3.413ms \n",
    "                                   aten::native_dropout      2.177ms     3.413ms \n",
    "                                             cudaMalloc      1.808ms     0.000us \n",
    "                                          aten::reshape      1.579ms    21.000us \n",
    "------------------------------------------------------- ------------ ----------- \n",
    "Self CPU time total: 1.621s\n",
    "Self CUDA time total: 252.116ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "for input_text in data['Sentence']:\n",
    "    num_predicted_tokens = 40\n",
    "    \n",
    "    with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        answer = predict_next(pipeline, input_text, num_predicted_tokens, tokenizer)\n",
    "\n",
    "    print('input:', input_text)\n",
    "    print('output:', answer[len(input_text)+6:])\n",
    "    print(\"-\"*40)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y.download(\"save_models/ethereal-planet-258/model_1\", \"model_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61.83308696746826"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3.128709316253662 + 2.2622079849243164 + 56.44216966629028"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (774384569.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[6], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    4090  - 22.64s\u001b[0m\n\u001b[1;37m                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "4090  - 22.64s,  159,  0.35$ - 0.22\n",
    "4080  - 36.83s,  97,   0.25$ - 0.25\n",
    "3090  - 37.22s,  96,   0.15$ - 0.15\n",
    "3080  - 56.07s,  64,   0.11$ - 0.17\n",
    "A4000 - 61.83s,  58,   0.11$ - 0.19\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1896551724137931"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.11 / 58 * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "8defd53a528e9245923bfdc9d5a38f5c3ca09cbc6e92fa1b95a37aeffbb04cd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
