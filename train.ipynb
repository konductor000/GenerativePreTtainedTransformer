{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "import torch.optim as optim\n",
    "\n",
    "from dataset.create_dataset import create_data_loader\n",
    "from layers.model import Transformer, AutoregressiveWrapper\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from test_model.test_model import TestModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "import time\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    \"architecture\": \"Transformer\", # Wandb only\n",
    "    \"dataset\": \"wikitext-103-raw-v1\", # Wandb only\n",
    "    \"batch_size\": 10,\n",
    "    \"accumulation\": 6,\n",
    "    \"embedding_size\": 768,\n",
    "    \"max_sequence_length\": 256,\n",
    "    \"number_of_layers\": 12,\n",
    "    \"number_of_heads\": 12,\n",
    "    \"additional_feed_forward_layers\": 0,\n",
    "    \"extention_factor\": 4,\n",
    "    \"attention_activation\": \"softmax\",\n",
    "    \"dropout_rate\": 0.1,\n",
    "    'train_size': 2**20,\n",
    "    'test_size': 128,\n",
    "    'model_path': None, # \"savepoints/likely-durian-120\"\n",
    "}\n",
    "CONFIG[\"lr\"] = 0.001 / np.sqrt(CONFIG[\"batch_size\"])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_model(pipeline, model, loss_function):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        model_output, target = pipeline(input_ids, attention_mask)\n",
    "\n",
    "        loss = loss_function(model_output.transpose(1, 2), target)\n",
    "\n",
    "        total_loss += float(loss)\n",
    "\n",
    "    total_loss /= len(test_dataloader)# * CONFIG[\"batch_size\"]\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def train(CONFIG, pipeline, model, optimizer, loss_function, model_tester, wandb):\n",
    "    train_config = {\n",
    "        \"test_every\": 1024 // CONFIG[\"batch_size\"],\n",
    "        \"log_traing_metrics_every\": 64 // CONFIG[\"batch_size\"],\n",
    "    }\n",
    "\n",
    "    train_time = 0\n",
    "    test_time = 0\n",
    "    last_moment = time.time()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_num = 0\n",
    "    train_losses = []\n",
    "    \n",
    "    for batch in tqdm(train_dataloader, desc=\"Training Progress\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        model_output, target = pipeline(input_ids, attention_mask)\n",
    "        loss = loss_function(model_output.transpose(1, 2), target)\n",
    "        train_losses.append(float(loss))\n",
    "        \n",
    "        loss = loss / CONFIG['accumulation']\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        \n",
    "        model_output.detach()\n",
    "        loss.detach()\n",
    "\n",
    "        if ((batch_num + 1) % CONFIG['accumulation'] == 0) or (batch_num + 1 == len(train_dataloader)):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        batch_num += 1\n",
    "\n",
    "        if batch_num % train_config[\"log_traing_metrics_every\"] == 0:\n",
    "            train_time += time.time() - last_moment\n",
    "            last_moment = time.time()\n",
    "\n",
    "            datapoints_processed_total = batch_num * CONFIG[\"batch_size\"]\n",
    "            wandb.log({\n",
    "                \"train_loss\": sum(train_losses[-train_config[\"log_traing_metrics_every\"]:]) / train_config[\"log_traing_metrics_every\"],\n",
    "                \"datapoints_processed_total\": datapoints_processed_total,\n",
    "                \"train_time\": train_time,\n",
    "            })\n",
    "\n",
    "        if batch_num % train_config[\"test_every\"] == 0:\n",
    "            train_time += time.time() - last_moment\n",
    "            last_moment = time.time()\n",
    "\n",
    "            metrics = model_tester.test_model(pipeline, test_dataloader)\n",
    "            test_loss = metrics['loss']\n",
    "            bleu = metrics['bleu']\n",
    "            #bert_f1 = metrics['bert_f1']\n",
    "            rouge1 = metrics['rouge1']\n",
    "            rouge2 = metrics['rouge2']\n",
    "            rougeL = metrics['rougeL']\n",
    "\n",
    "            test_time += time.time() - last_moment\n",
    "            last_moment = time.time()\n",
    "\n",
    "            datapoints_processed_total = batch_num * CONFIG[\"batch_size\"]\n",
    "\n",
    "            wandb.log({\n",
    "                \"test_loss\": test_loss,\n",
    "                \"bleu\": bleu,\n",
    "                #\"bert_f1\": bert_f1,\n",
    "                \"rouge1\": rouge1,\n",
    "                \"rouge2\": rouge2,\n",
    "                \"rougeL\": rougeL,\n",
    "                \"datapoints_processed_total\": datapoints_processed_total,\n",
    "                \"test_time\": test_time,\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(CONFIG, model_path=None):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    number_of_tokens = tokenizer.vocab_size\n",
    "    \n",
    "    model = Transformer(\n",
    "        embedding_size=CONFIG[\"embedding_size\"],\n",
    "        number_of_tokens=number_of_tokens,\n",
    "        number_of_heads=CONFIG[\"number_of_heads\"],\n",
    "        number_of_layers=CONFIG[\"number_of_layers\"],\n",
    "        extention_factor=CONFIG[\"extention_factor\"],\n",
    "        additional_feed_forward_layers=CONFIG[\"additional_feed_forward_layers\"],\n",
    "        attention_activation=CONFIG[\"attention_activation\"],\n",
    "        dropout_rate=CONFIG[\"dropout_rate\"],\n",
    "        max_sequence_length=CONFIG[\"max_sequence_length\"]\n",
    "    ).to(device)\n",
    "    if model_path:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        \n",
    "    pipeline = AutoregressiveWrapper(model).to(device)\n",
    "    loss_function = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
    "    model_tester = TestModel(tokenizer, model)\n",
    "\n",
    "    return pipeline, model, optimizer, loss_function, model_tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (C:/Users/skoro/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "021e116c571a4c67b560f56bf31834ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\skoro\\.cache\\huggingface\\datasets\\wikitext\\wikitext-103-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-be33ed5b4b4522e7.arrow\n",
      "Loading cached processed dataset at C:\\Users\\skoro\\.cache\\huggingface\\datasets\\wikitext\\wikitext-103-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-26a133c3c002dbd8.arrow\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskorodumov-work\u001b[0m (\u001b[33m8667\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\skoro\\VS_projects\\Transformer\\wandb\\run-20230814_234125-2txopp5y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/8667/transformer/runs/2txopp5y' target=\"_blank\">fancy-snow-158</a></strong> to <a href='https://wandb.ai/8667/transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/8667/transformer' target=\"_blank\">https://wandb.ai/8667/transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/8667/transformer/runs/2txopp5y' target=\"_blank\">https://wandb.ai/8667/transformer/runs/2txopp5y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters = 131968314\n",
      "number of trainable parameters = 131968314\n",
      "memory allocated in GB = 0.4916202798485756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   2%|▍                      | 2121/104858 [23:40<18:22:18,  1.55it/s]"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    train_dataloader, test_dataloader, _ = create_data_loader(batch_size=CONFIG[\"batch_size\"],\n",
    "                                    max_sequence_size=CONFIG[\"max_sequence_length\"],\n",
    "                                    train_size=CONFIG['train_size'], test_size=CONFIG['test_size'])\n",
    "\n",
    "    wandb_run = wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"transformer\",\n",
    "        tags=[\"long_training_testing\"],\n",
    "        \n",
    "        # track hyperparameters and run metadata\n",
    "        config=CONFIG\n",
    "    )\n",
    "    \n",
    "    load_path = CONFIG['model_path']\n",
    "\n",
    "    pipeline, model, optimizer, loss_function, model_tester = create_model(CONFIG, load_path)\n",
    "    num_parameters, num_trainable_parameters, memory_allocated = pipeline.count_parameters() \n",
    "    print('number of parameters =', num_parameters)\n",
    "    print('number of trainable parameters =', num_trainable_parameters)\n",
    "    print('memory allocated in GB =', memory_allocated)\n",
    "    prof = train(CONFIG, pipeline, model, optimizer, loss_function, model_tester, wandb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"savepoints/\" + wandb_run.name\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline, model, optimizer, loss_function, model_tester = create_model(CONFIG, CONFIG['model_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (C:/Users/skoro/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72556654133f43f09197ef1548077135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\skoro\\.cache\\huggingface\\datasets\\wikitext\\wikitext-103-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-be33ed5b4b4522e7.arrow\n",
      "Loading cached processed dataset at C:\\Users\\skoro\\.cache\\huggingface\\datasets\\wikitext\\wikitext-103-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-26a133c3c002dbd8.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, test_dataloader, _ = create_data_loader(batch_size=CONFIG[\"batch_size\"],\n",
    "                                    max_sequence_size=CONFIG[\"max_sequence_length\"],\n",
    "                                    train_size=CONFIG['train_size'], test_size=CONFIG['test_size'])\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 256])\n"
     ]
    }
   ],
   "source": [
    "with profile(activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA], record_shapes=True) as prof:\n",
    "    i = 0\n",
    "    for batch in train_dataloader:\n",
    "        i += 1\n",
    "\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        \n",
    "        print(input_ids.size())\n",
    "\n",
    "        with record_function(\"model_inference\"):\n",
    "            model_output, target = pipeline(input_ids, attention_mask)\n",
    "\n",
    "        if i == 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                        model_inference        24.75%      31.094ms        98.50%     123.752ms     123.752ms       4.887ms         2.44%     199.077ms     199.077ms             1  \n",
      "                                           aten::linear         7.32%       9.202ms        27.97%      35.146ms      74.938us       4.364ms         2.18%     118.161ms     251.942us           469  \n",
      "                                            aten::addmm        11.76%      14.781ms        13.69%      17.198ms      36.670us     106.835ms        53.31%     108.917ms     232.232us           469  \n",
      "                                           aten::matmul         6.70%       8.419ms        19.93%      25.042ms      86.951us       3.189ms         1.59%      20.345ms      70.642us           288  \n",
      "                                      aten::masked_fill         2.56%       3.218ms         9.28%      11.657ms      80.951us       1.305ms         0.65%      14.171ms      98.410us           144  \n",
      "                                              aten::bmm         4.73%       5.937ms         4.73%       5.937ms      20.615us      11.271ms         5.62%      11.271ms      39.135us           288  \n",
      "                                              aten::sub         2.30%       2.887ms         2.30%       2.887ms      10.024us       9.934ms         4.96%       9.934ms      34.493us           288  \n",
      "                                           aten::expand         6.88%       8.645ms         7.23%       9.082ms       6.149us       4.384ms         2.19%       6.691ms       4.530us          1477  \n",
      "                                     aten::masked_fill_         1.66%       2.085ms         2.25%       2.826ms      19.625us       5.498ms         2.74%       6.153ms      42.729us           144  \n",
      "                                             aten::tril         1.07%       1.345ms         1.07%       1.345ms       9.340us       5.948ms         2.97%       5.948ms      41.306us           144  \n",
      "                                            aten::clone         1.15%       1.445ms         3.35%       4.213ms      29.055us     647.000us         0.32%       5.442ms      37.531us           145  \n",
      "                                            aten::copy_         1.63%       2.051ms         1.63%       2.051ms      13.765us       5.083ms         2.54%       5.083ms      34.114us           149  \n",
      "                                             aten::rsub         0.75%     946.000us         1.84%       2.308ms      16.028us     428.000us         0.21%       4.786ms      33.236us           144  \n",
      "                                          aten::softmax         0.57%     716.000us         1.82%       2.291ms      15.910us     425.000us         0.21%       4.265ms      29.618us           144  \n",
      "                                         aten::_softmax         1.25%       1.575ms         1.25%       1.575ms      10.938us       3.840ms         1.92%       3.840ms      26.667us           144  \n",
      "                                              aten::mul         0.94%       1.185ms         0.94%       1.185ms       8.229us       3.594ms         1.79%       3.594ms      24.958us           144  \n",
      "                                                aten::t         2.69%       3.379ms         4.81%       6.037ms      12.872us       1.402ms         0.70%       3.477ms       7.414us           469  \n",
      "                                       aten::as_strided         0.99%       1.239ms         0.99%       1.239ms       0.548us       3.476ms         1.73%       3.476ms       1.537us          2262  \n",
      "                                              aten::div         1.54%       1.941ms         1.54%       1.941ms      13.479us       3.397ms         1.69%       3.397ms      23.590us           144  \n",
      "                                        aten::ones_like         1.04%       1.309ms         2.61%       3.282ms      22.792us     650.000us         0.32%       3.219ms      22.354us           144  \n",
      "                                              aten::add         0.25%     318.000us         0.25%     318.000us      12.720us       2.853ms         1.42%       2.853ms     114.120us            25  \n",
      "                                          aten::reshape         3.00%       3.767ms         4.33%       5.436ms       9.421us       1.915ms         0.96%       2.799ms       4.851us           577  \n",
      "                                       aten::layer_norm         0.11%     138.000us         1.00%       1.255ms      50.200us      71.000us         0.04%       2.734ms     109.360us            25  \n",
      "                                        aten::transpose         2.55%       3.202ms         3.02%       3.799ms       6.197us       1.797ms         0.90%       2.708ms       4.418us           613  \n",
      "                                aten::native_layer_norm         0.72%     907.000us         0.89%       1.117ms      44.680us       2.478ms         1.24%       2.663ms     106.520us            25  \n",
      "                                            aten::fill_         0.54%     680.000us         0.54%     680.000us       4.722us       1.923ms         0.96%       1.923ms      13.354us           144  \n",
      "                                       aten::empty_like         1.30%       1.635ms         2.07%       2.596ms       8.294us       1.130ms         0.56%       1.596ms       5.099us           313  \n",
      "                                          aten::dropout         0.06%      75.000us         0.47%     587.000us      48.917us      34.000us         0.02%       1.520ms     126.667us            12  \n",
      "                                   aten::native_dropout         0.26%     324.000us         0.41%     512.000us      42.667us       1.381ms         0.69%       1.486ms     123.833us            12  \n",
      "                                             aten::view         2.18%       2.743ms         2.18%       2.743ms       2.768us       1.482ms         0.74%       1.482ms       1.495us           991  \n",
      "                                               aten::eq         1.44%       1.809ms         1.44%       1.809ms      12.562us       1.294ms         0.65%       1.294ms       8.986us           144  \n",
      "                                               aten::to         0.02%      21.000us         0.39%     491.000us      98.200us      14.000us         0.01%       1.170ms     234.000us             5  \n",
      "                                         aten::_to_copy         0.04%      51.000us         0.37%     470.000us     117.500us      18.000us         0.01%       1.156ms     289.000us             4  \n",
      "                                              aten::cat         0.29%     368.000us         0.32%     401.000us      28.643us     895.000us         0.45%     910.000us      65.000us            14  \n",
      "                                   aten::_reshape_alias         1.29%       1.623ms         1.29%       1.623ms       2.818us     867.000us         0.43%     867.000us       1.505us           576  \n",
      "                                        aten::unsqueeze         0.89%       1.119ms         1.05%       1.315ms       9.132us     426.000us         0.21%     639.000us       4.438us           144  \n",
      "                                     aten::_unsafe_view         0.66%     831.000us         0.66%     831.000us       2.875us     440.000us         0.22%     440.000us       1.522us           289  \n",
      "                                            aten::empty         0.48%     598.000us         0.48%     598.000us       2.694us     319.000us         0.16%     319.000us       1.437us           222  \n",
      "                                    aten::empty_strided         0.46%     576.000us         0.46%     576.000us       3.349us     266.000us         0.13%     266.000us       1.547us           172  \n",
      "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...         0.88%       1.108ms         1.12%       1.410ms       1.410ms      45.000us         0.02%     188.000us     188.000us             1  \n",
      "                                        aten::embedding         0.03%      32.000us         0.10%     121.000us     121.000us       6.000us         0.00%     123.000us     123.000us             1  \n",
      "                                     aten::index_select         0.02%      23.000us         0.02%      29.000us      29.000us      93.000us         0.05%      95.000us      95.000us             1  \n",
      "                                           aten::select         0.08%     104.000us         0.09%     107.000us       5.944us      48.000us         0.02%      78.000us       4.333us            18  \n",
      "                                            aten::slice         0.06%      75.000us         0.06%      81.000us       8.100us      29.000us         0.01%      44.000us       4.400us            10  \n",
      "                                            aten::stack         0.03%      38.000us         0.10%     122.000us      61.000us       9.000us         0.00%      32.000us      16.000us             2  \n",
      "                                           aten::narrow         0.01%      14.000us         0.03%      33.000us      16.500us       7.000us         0.00%      15.000us       7.500us             2  \n",
      "                                          aten::detach_         0.01%       7.000us         0.01%      10.000us       5.000us       7.000us         0.00%       9.000us       4.500us             2  \n",
      "                                             aten::item         0.01%      11.000us         0.01%      14.000us      14.000us       4.000us         0.00%       5.000us       5.000us             1  \n",
      "                                           aten::detach         0.00%       4.000us         0.01%       7.000us       7.000us       4.000us         0.00%       5.000us       5.000us             1  \n",
      "                                       aten::lift_fresh         0.00%       3.000us         0.00%       3.000us       1.500us       3.000us         0.00%       3.000us       1.500us             2  \n",
      "                                                detach_         0.00%       3.000us         0.00%       3.000us       1.500us       2.000us         0.00%       2.000us       1.000us             2  \n",
      "                                          aten::random_         0.02%      19.000us         0.02%      19.000us      19.000us       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                              aten::_local_scalar_dense         0.00%       3.000us         0.00%       3.000us       3.000us       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                                          aten::resize_         0.00%       5.000us         0.00%       5.000us       5.000us       1.000us         0.00%       1.000us       1.000us             1  \n",
      "                                                 detach         0.00%       3.000us         0.00%       3.000us       3.000us       1.000us         0.00%       1.000us       1.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 125.636ms\n",
      "Self CUDA time total: 200.421ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def predict_next(pipeline, input_text, num_predicted_tokens, tokenizer):\n",
    "    input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    input_tokens = input_tokens[:, :-1].to(device)\n",
    "    pipeline.eval()\n",
    "    \n",
    "    for i in range(num_predicted_tokens):\n",
    "        mask = torch.ones_like(input_tokens)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probabilities = pipeline.next_token_probabilities(input_tokens, mask)\n",
    "        \n",
    "        answer = probabilities.argsort(dim=-1)[:, -randint(1, 2)].unsqueeze(0)\n",
    "        input_tokens = torch.cat((input_tokens, answer), dim=1)\n",
    "        \n",
    "    return tokenizer.decode(input_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "london is the capital of\n",
    "\"\"\"\n",
    "num_predicted_tokens = 50\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "answer = predict_next(pipeline, input_text, num_predicted_tokens, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"[CLS] london is the capital of the country, and is the home to a family of local residents. [SEP] are a family, but only the home is a family. [SEP] are the family's home, and the home is a small room, which is a small room. the\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Performance \n",
    "first approach\n",
    "batch_size, accumulation, num DP, time\n",
    "4           4             2**9    40s\n",
    "8           1             2**9    37s\n",
    "6           4             2**9    34s\n",
    "4           6             2**9    39s\n",
    "5           5             2**9    37s\n",
    "8           2             2**9    34s\n",
    "10          2             2**9    33s\n",
    "8           3             2**9    33s\n",
    "11          1             2**9    34s\n",
    "14          1             2**9    34s\n",
    "second approach\n",
    "batch_size, accumulation, num DP, time\n",
    "14          1             2**9    32s\n",
    "8           3             2**9    34s\n",
    "8           4             2**9    33s\n",
    "10          4             2**9    33s\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GPU comparison\n",
    "1. GOOGLE COLAB\n",
    "-100 CU are given for 10 dollars\n",
    "-V100, 32 GB, 5 CU per hour, 20 hours, 706 DP, 14120 DP total, 1412 DP per dollar\n",
    "-A100, 40 GB, 15 CU per hour, 6.5 hours, 2179 DP, 14163 DP total, 1416 DP per dollar\n",
    "\n",
    "2. VAST.AI\n",
    "-RTX 4090, 24 GB, 0.4 dollars per hour, 1720 DP, 4300 DP per dollar, 177% while training on 4 GPUs\n",
    "-RTX 3080, 10 GB, 0.117 dollars per hour, 1022 DP, 8717 DP per dollar, 177% while training on 4 GPUs\n",
    "-RTX 3090, 24 GB, 0.23 dollars per hour, 1071 DP, 4652 DP per dollar, 177% while training on 4 GPUs\n",
    "-A5000, 24 GB, 0.22 dollars per hour, 1061 DP, 4686 DP per dollar, 389% while training on 4 GPUs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO\n",
    "optimize\n",
    "-understand time wasting\n",
    "-choose batch size\n",
    "\n",
    "\n",
    "beam search\n",
    "\n",
    "new dataset\n",
    "-find new dataset for big training\n",
    "-make some analysis\n",
    "-avg, mean, each percentile sequence length\n",
    "-find info in the net\n",
    "\n",
    "model parameters\n",
    "-max sequence length\n",
    "-estimate batch size\n",
    "-choose parameters for n GB VRAM\n",
    "-choose model parameters for 5-6 GB model\n",
    "\n",
    "filter model\n",
    "-\n",
    "\n",
    "new conversation dataset \n",
    "-analyse\n",
    "-understand how to train and test conversation model\n",
    "-try to train\n",
    "\n",
    "metrics\n",
    "-create metrics for conversation\n",
    "\n",
    "multiple GPUs\n",
    "-write code to run model on several gpus\n",
    "-try to run model on vast.ai\n",
    "-also model must be saved per epoch\n",
    "\n",
    "site\n",
    "-create django project with model which is placed on google colab\n",
    "-create \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
    "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
    "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
    "                                        model_inference        24.75%      31.094ms        98.50%     123.752ms     123.752ms       4.887ms         2.44%     199.077ms     199.077ms             1  \n",
    "                                           aten::linear         7.32%       9.202ms        27.97%      35.146ms      74.938us       4.364ms         2.18%     118.161ms     251.942us           469  \n",
    "                                            aten::addmm        11.76%      14.781ms        13.69%      17.198ms      36.670us     106.835ms        53.31%     108.917ms     232.232us           469  \n",
    "                                           aten::matmul         6.70%       8.419ms        19.93%      25.042ms      86.951us       3.189ms         1.59%      20.345ms      70.642us           288  \n",
    "                                      aten::masked_fill         2.56%       3.218ms         9.28%      11.657ms      80.951us       1.305ms         0.65%      14.171ms      98.410us           144  \n",
    "                                              aten::bmm         4.73%       5.937ms         4.73%       5.937ms      20.615us      11.271ms         5.62%      11.271ms      39.135us           288  \n",
    "                                              aten::sub         2.30%       2.887ms         2.30%       2.887ms      10.024us       9.934ms         4.96%       9.934ms      34.493us           288  \n",
    "                                           aten::expand         6.88%       8.645ms         7.23%       9.082ms       6.149us       4.384ms         2.19%       6.691ms       4.530us          1477  \n",
    "                                     aten::masked_fill_         1.66%       2.085ms         2.25%       2.826ms      19.625us       5.498ms         2.74%       6.153ms      42.729us           144  \n",
    "                                             aten::tril         1.07%       1.345ms         1.07%       1.345ms       9.340us       5.948ms         2.97%       5.948ms      41.306us           144  \n",
    "                                            aten::clone         1.15%       1.445ms         3.35%       4.213ms      29.055us     647.000us         0.32%       5.442ms      37.531us           145  \n",
    "                                            aten::copy_         1.63%       2.051ms         1.63%       2.051ms      13.765us       5.083ms         2.54%       5.083ms      34.114us           149  \n",
    "                                             aten::rsub         0.75%     946.000us         1.84%       2.308ms      16.028us     428.000us         0.21%       4.786ms      33.236us           144  \n",
    "                                          aten::softmax         0.57%     716.000us         1.82%       2.291ms      15.910us     425.000us         0.21%       4.265ms      29.618us           144  \n",
    "                                         aten::_softmax         1.25%       1.575ms         1.25%       1.575ms      10.938us       3.840ms         1.92%       3.840ms      26.667us           144  \n",
    "                                              aten::mul         0.94%       1.185ms         0.94%       1.185ms       8.229us       3.594ms         1.79%       3.594ms      24.958us           144  \n",
    "                                                aten::t         2.69%       3.379ms         4.81%       6.037ms      12.872us       1.402ms         0.70%       3.477ms       7.414us           469  \n",
    "                                       aten::as_strided         0.99%       1.239ms         0.99%       1.239ms       0.548us       3.476ms         1.73%       3.476ms       1.537us          2262  \n",
    "                                              aten::div         1.54%       1.941ms         1.54%       1.941ms      13.479us       3.397ms         1.69%       3.397ms      23.590us           144  \n",
    "                                        aten::ones_like         1.04%       1.309ms         2.61%       3.282ms      22.792us     650.000us         0.32%       3.219ms      22.354us           144  \n",
    "                                              aten::add         0.25%     318.000us         0.25%     318.000us      12.720us       2.853ms         1.42%       2.853ms     114.120us            25  \n",
    "                                          aten::reshape         3.00%       3.767ms         4.33%       5.436ms       9.421us       1.915ms         0.96%       2.799ms       4.851us           577  \n",
    "                                       aten::layer_norm         0.11%     138.000us         1.00%       1.255ms      50.200us      71.000us         0.04%       2.734ms     109.360us            25  \n",
    "                                        aten::transpose         2.55%       3.202ms         3.02%       3.799ms       6.197us       1.797ms         0.90%       2.708ms       4.418us           613  \n",
    "                                aten::native_layer_norm         0.72%     907.000us         0.89%       1.117ms      44.680us       2.478ms         1.24%       2.663ms     106.520us            25  \n",
    "                                            aten::fill_         0.54%     680.000us         0.54%     680.000us       4.722us       1.923ms         0.96%       1.923ms      13.354us           144  \n",
    "                                       aten::empty_like         1.30%       1.635ms         2.07%       2.596ms       8.294us       1.130ms         0.56%       1.596ms       5.099us           313  \n",
    "                                          aten::dropout         0.06%      75.000us         0.47%     587.000us      48.917us      34.000us         0.02%       1.520ms     126.667us            12  \n",
    "                                   aten::native_dropout         0.26%     324.000us         0.41%     512.000us      42.667us       1.381ms         0.69%       1.486ms     123.833us            12  \n",
    "                                             aten::view         2.18%       2.743ms         2.18%       2.743ms       2.768us       1.482ms         0.74%       1.482ms       1.495us           991  \n",
    "                                               aten::eq         1.44%       1.809ms         1.44%       1.809ms      12.562us       1.294ms         0.65%       1.294ms       8.986us           144  \n",
    "                                               aten::to         0.02%      21.000us         0.39%     491.000us      98.200us      14.000us         0.01%       1.170ms     234.000us             5  \n",
    "                                         aten::_to_copy         0.04%      51.000us         0.37%     470.000us     117.500us      18.000us         0.01%       1.156ms     289.000us             4  \n",
    "                                              aten::cat         0.29%     368.000us         0.32%     401.000us      28.643us     895.000us         0.45%     910.000us      65.000us            14  \n",
    "                                   aten::_reshape_alias         1.29%       1.623ms         1.29%       1.623ms       2.818us     867.000us         0.43%     867.000us       1.505us           576  \n",
    "                                        aten::unsqueeze         0.89%       1.119ms         1.05%       1.315ms       9.132us     426.000us         0.21%     639.000us       4.438us           144  \n",
    "                                     aten::_unsafe_view         0.66%     831.000us         0.66%     831.000us       2.875us     440.000us         0.22%     440.000us       1.522us           289  \n",
    "                                            aten::empty         0.48%     598.000us         0.48%     598.000us       2.694us     319.000us         0.16%     319.000us       1.437us           222  \n",
    "                                    aten::empty_strided         0.46%     576.000us         0.46%     576.000us       3.349us     266.000us         0.13%     266.000us       1.547us           172  \n",
    "enumerate(DataLoader)#_SingleProcessDataLoaderIter._...         0.88%       1.108ms         1.12%       1.410ms       1.410ms      45.000us         0.02%     188.000us     188.000us             1  \n",
    "                                        aten::embedding         0.03%      32.000us         0.10%     121.000us     121.000us       6.000us         0.00%     123.000us     123.000us             1  \n",
    "                                     aten::index_select         0.02%      23.000us         0.02%      29.000us      29.000us      93.000us         0.05%      95.000us      95.000us             1  \n",
    "                                           aten::select         0.08%     104.000us         0.09%     107.000us       5.944us      48.000us         0.02%      78.000us       4.333us            18  \n",
    "                                            aten::slice         0.06%      75.000us         0.06%      81.000us       8.100us      29.000us         0.01%      44.000us       4.400us            10  \n",
    "                                            aten::stack         0.03%      38.000us         0.10%     122.000us      61.000us       9.000us         0.00%      32.000us      16.000us             2  \n",
    "                                           aten::narrow         0.01%      14.000us         0.03%      33.000us      16.500us       7.000us         0.00%      15.000us       7.500us             2  \n",
    "                                          aten::detach_         0.01%       7.000us         0.01%      10.000us       5.000us       7.000us         0.00%       9.000us       4.500us             2  \n",
    "                                             aten::item         0.01%      11.000us         0.01%      14.000us      14.000us       4.000us         0.00%       5.000us       5.000us             1  \n",
    "                                           aten::detach         0.00%       4.000us         0.01%       7.000us       7.000us       4.000us         0.00%       5.000us       5.000us             1  \n",
    "                                       aten::lift_fresh         0.00%       3.000us         0.00%       3.000us       1.500us       3.000us         0.00%       3.000us       1.500us             2  \n",
    "                                                detach_         0.00%       3.000us         0.00%       3.000us       1.500us       2.000us         0.00%       2.000us       1.000us             2  \n",
    "                                          aten::random_         0.02%      19.000us         0.02%      19.000us      19.000us       1.000us         0.00%       1.000us       1.000us             1  \n",
    "                              aten::_local_scalar_dense         0.00%       3.000us         0.00%       3.000us       3.000us       1.000us         0.00%       1.000us       1.000us             1  \n",
    "                                          aten::resize_         0.00%       5.000us         0.00%       5.000us       5.000us       1.000us         0.00%       1.000us       1.000us             1  \n",
    "                                                 detach         0.00%       3.000us         0.00%       3.000us       3.000us       1.000us         0.00%       1.000us       1.000us             1  \n",
    "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
    "Self CPU time total: 125.636ms\n",
    "Self CUDA time total: 200.421ms\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "8defd53a528e9245923bfdc9d5a38f5c3ca09cbc6e92fa1b95a37aeffbb04cd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
