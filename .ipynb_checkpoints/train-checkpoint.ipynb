{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "import torch.optim as optim\n",
    "\n",
    "from dataset.create_dataset import create_data_loader\n",
    "from layers.model import Transformer, AutoregressiveWrapper\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from test_model.test_model import TestModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    \"architecture\": \"Transformer\", # Wandb only\n",
    "    \"dataset\": \"wikitext-103-raw-v1\", # Wandb only\n",
    "    \"batch_size\": 10,\n",
    "    \"embedding_size\": 768,\n",
    "    \"max_sequence_length\": 256,\n",
    "    \"number_of_layers\": 12,\n",
    "    \"number_of_heads\": 12,\n",
    "    \"additional_feed_forward_layers\": 0,\n",
    "    \"extention_factor\": 4,\n",
    "    \"attention_activation\": \"softmax\",\n",
    "    \"dropout_rate\": 0.1,\n",
    "    'train_size': 2**20,\n",
    "    'test_size': 128,\n",
    "    'model_path': \"savepoints/dazzling-planet-115\"\n",
    "}\n",
    "CONFIG[\"lr\"] = 0.001 / np.sqrt(CONFIG[\"batch_size\"])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def test_model(pipeline, model, loss_function):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        model_output, target = pipeline(input_ids, attention_mask)\n",
    "\n",
    "        loss = loss_function(model_output.transpose(1, 2), target)\n",
    "\n",
    "        total_loss += float(loss)\n",
    "\n",
    "    total_loss /= len(test_dataloader)# * CONFIG[\"batch_size\"]\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def train(CONFIG, pipeline, model, optimizer, loss_function, model_tester, wandb):\n",
    "    train_config = {\n",
    "        \"test_every\": 1024 // CONFIG[\"batch_size\"],\n",
    "        \"log_traing_metrics_every\": 64 // CONFIG[\"batch_size\"],\n",
    "    }\n",
    "\n",
    "    train_time = 0\n",
    "    test_time = 0\n",
    "    last_moment = time.time()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_num = 0\n",
    "    train_losses = []\n",
    "    for batch in tqdm(train_dataloader, desc=\"Training Progress\"):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        model_output, target = pipeline(input_ids, attention_mask)\n",
    "        loss = loss_function(model_output.transpose(1, 2), target)\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(float(loss))\n",
    "        batch_num += 1\n",
    "\n",
    "        if batch_num % train_config[\"log_traing_metrics_every\"] == 0:\n",
    "            train_time += time.time() - last_moment\n",
    "            last_moment = time.time()\n",
    "\n",
    "            datapoints_processed_total = batch_num * CONFIG[\"batch_size\"]\n",
    "            wandb.log({\n",
    "                \"train_loss\": sum(train_losses[-train_config[\"log_traing_metrics_every\"]:]) / train_config[\"log_traing_metrics_every\"],\n",
    "                \"datapoints_processed_total\": datapoints_processed_total,\n",
    "                \"train_time\": train_time,\n",
    "            })\n",
    "\n",
    "        if batch_num % train_config[\"test_every\"] == 0:\n",
    "            train_time += time.time() - last_moment\n",
    "            last_moment = time.time()\n",
    "\n",
    "            metrics = model_tester.test_model(pipeline, test_dataloader)\n",
    "            test_loss = metrics['loss']\n",
    "            bleu = metrics['bleu']\n",
    "            #bert_f1 = metrics['bert_f1']\n",
    "            rouge1 = metrics['rouge1']\n",
    "            rouge2 = metrics['rouge2']\n",
    "            rougeL = metrics['rougeL']\n",
    "\n",
    "            test_time += time.time() - last_moment\n",
    "            last_moment = time.time()\n",
    "\n",
    "            datapoints_processed_total = batch_num * CONFIG[\"batch_size\"]\n",
    "\n",
    "            wandb.log({\n",
    "                \"test_loss\": test_loss,\n",
    "                \"bleu\": bleu,\n",
    "                #\"bert_f1\": bert_f1,\n",
    "                \"rouge1\": rouge1,\n",
    "                \"rouge2\": rouge2,\n",
    "                \"rougeL\": rougeL,\n",
    "                \"datapoints_processed_total\": datapoints_processed_total,\n",
    "                \"test_time\": test_time,\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(CONFIG, model_path=None):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    number_of_tokens = tokenizer.vocab_size\n",
    "    \n",
    "    model = Transformer(\n",
    "        embedding_size=CONFIG[\"embedding_size\"],\n",
    "        number_of_tokens=number_of_tokens,\n",
    "        number_of_heads=CONFIG[\"number_of_heads\"],\n",
    "        number_of_layers=CONFIG[\"number_of_layers\"],\n",
    "        extention_factor=CONFIG[\"extention_factor\"],\n",
    "        additional_feed_forward_layers=CONFIG[\"additional_feed_forward_layers\"],\n",
    "        attention_activation=CONFIG[\"attention_activation\"],\n",
    "        dropout_rate=CONFIG[\"dropout_rate\"],\n",
    "        max_sequence_length=CONFIG[\"max_sequence_length\"]\n",
    "    ).to(device)\n",
    "    if model_path:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        \n",
    "    pipeline = AutoregressiveWrapper(model).to(device)\n",
    "    loss_function = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
    "    model_tester = TestModel(tokenizer, model)\n",
    "\n",
    "    return pipeline, model, optimizer, loss_function, model_tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikitext (C:/Users/skoro/.cache/huggingface/datasets/wikitext/wikitext-103-raw-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae28a79eea047929608f8c3b31ff1be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\skoro\\.cache\\huggingface\\datasets\\wikitext\\wikitext-103-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-be33ed5b4b4522e7.arrow\n",
      "Loading cached processed dataset at C:\\Users\\skoro\\.cache\\huggingface\\datasets\\wikitext\\wikitext-103-raw-v1\\1.0.0\\a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126\\cache-26a133c3c002dbd8.arrow\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskorodumov-work\u001b[0m (\u001b[33m8667\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\skoro\\VS_projects\\Transformer\\wandb\\run-20230806_182407-wf0t2xme</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/8667/transformer/runs/wf0t2xme' target=\"_blank\">zesty-dew-116</a></strong> to <a href='https://wandb.ai/8667/transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/8667/transformer' target=\"_blank\">https://wandb.ai/8667/transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/8667/transformer/runs/wf0t2xme' target=\"_blank\">https://wandb.ai/8667/transformer/runs/wf0t2xme</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters = 131968314\n",
      "number of trainable parameters = 131968314\n",
      "memory allocated in GB = 0.4916202798485756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   6%|▊           | 6660/104858 [1:26:55<21:21:33,  1.28it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber of trainable parameters =\u001b[39m\u001b[38;5;124m'\u001b[39m, num_trainable_parameters)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmemory allocated in GB =\u001b[39m\u001b[38;5;124m'\u001b[39m, memory_allocated)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_tester\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 43\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(CONFIG, pipeline, model, optimizer, loss_function, model_tester, wandb)\u001b[0m\n\u001b[0;32m     40\u001b[0m model_output, target \u001b[38;5;241m=\u001b[39m pipeline(input_ids, attention_mask)\n\u001b[0;32m     41\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_function(model_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), target)\n\u001b[1;32m---> 43\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m0.5\u001b[39m)\n\u001b[0;32m     45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\users\\skoro\\vs_projects\\virtualenvs\\torchvenv\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\skoro\\vs_projects\\virtualenvs\\torchvenv\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    train_dataloader, test_dataloader, _ = create_data_loader(batch_size=CONFIG[\"batch_size\"],\n",
    "                                    max_sequence_size=CONFIG[\"max_sequence_length\"],\n",
    "                                    train_size=CONFIG['train_size'], test_size=CONFIG['test_size'])\n",
    "\n",
    "    wandb_run = wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"transformer\",\n",
    "        tags=[\"long_training_testing\"],\n",
    "        \n",
    "        # track hyperparameters and run metadata\n",
    "        config=CONFIG\n",
    "    )\n",
    "    \n",
    "    load_path = CONFIG['model_path']\n",
    "\n",
    "    pipeline, model, optimizer, loss_function, model_tester = create_model(CONFIG, load_path)\n",
    "    num_parameters, num_trainable_parameters, memory_allocated = pipeline.count_parameters() \n",
    "    print('number of parameters =', num_parameters)\n",
    "    print('number of trainable parameters =', num_trainable_parameters)\n",
    "    print('memory allocated in GB =', memory_allocated)\n",
    "    train(CONFIG, pipeline, model, optimizer, loss_function, model_tester, wandb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"savepoints/\" + wandb_run.name\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wf0t2xme) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>bleu</td><td>▅▅▆▇█▁▇▇▇▅▆▄▇▄▆▄█▆▆▇█▅▆▄▆▇█▇▅▆▇▇▇▇▄▇▇▇▇█</td></tr><tr><td>datapoints_processed_total</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>rouge1</td><td>▅▃▄▄▆▂▅▅▄▄█▁█▄▅▆▆▇▅▅▇▄▅▄▄▆█▁▄▂▅▇▆▅▃█▄▆▅▇</td></tr><tr><td>rouge2</td><td>▆▅▆▆▆▁▄▄▄▄▇▄▇▁▅▇▇▅▅▆█▄▆▃▄▅▇▄▃▆▇▅▄▅▂█▅▆▇█</td></tr><tr><td>rougeL</td><td>▇▅▆▅▇▁▄▅▄▃█▃▇▂▅▆▇▅▅▆█▅▆▅▅▅▇▃▄▄▇▅▅▅▃▇▆▅▆▇</td></tr><tr><td>test_loss</td><td>▄▄▅▄▃▃▃▄▃▃▄▆▃▃▄▃▄▃▃▃▂▃▃▄▃▂▂█▃█▂▁▂▃▃▂▂▁▄▂</td></tr><tr><td>test_time</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>▄▇▄▄▅▄▃▄▂▆▄▃▆▄▄▆▃▃▆▄▄▅▇█▄▃▄▄▂▂▅▅▄▁▄▅▄▂▃▃</td></tr><tr><td>train_time</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>bleu</td><td>0.20103</td></tr><tr><td>datapoints_processed_total</td><td>66600</td></tr><tr><td>rouge1</td><td>0.64364</td></tr><tr><td>rouge2</td><td>0.49026</td></tr><tr><td>rougeL</td><td>0.60185</td></tr><tr><td>test_loss</td><td>1.0666</td></tr><tr><td>test_time</td><td>664.14055</td></tr><tr><td>train_loss</td><td>0.47208</td></tr><tr><td>train_time</td><td>4550.53756</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">zesty-dew-116</strong> at: <a href='https://wandb.ai/8667/transformer/runs/wf0t2xme' target=\"_blank\">https://wandb.ai/8667/transformer/runs/wf0t2xme</a><br/> View job at <a href='https://wandb.ai/8667/transformer/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjg2ODg2MjU0/version_details/v11' target=\"_blank\">https://wandb.ai/8667/transformer/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjg2ODg2MjU0/version_details/v11</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230806_182407-wf0t2xme\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:wf0t2xme). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4cb8038690d43739763b97aaad5330d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333435779, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.8 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\skoro\\VS_projects\\Transformer\\wandb\\run-20230806_195611-6agaal33</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/8667/transformer/runs/6agaal33' target=\"_blank\">helpful-wave-117</a></strong> to <a href='https://wandb.ai/8667/transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/8667/transformer' target=\"_blank\">https://wandb.ai/8667/transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/8667/transformer/runs/6agaal33' target=\"_blank\">https://wandb.ai/8667/transformer/runs/6agaal33</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<wandb.sdk.wandb_run.Run object at 0x0000013661B90910>\n"
     ]
    }
   ],
   "source": [
    "result = wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"transformer\",\n",
    "        tags=[\"long_training_testing\"],\n",
    "        \n",
    "        # track hyperparameters and run metadata\n",
    "        config=CONFIG\n",
    "    )\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'helpful-wave-117'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>bleu</td><td>▁▆▆▆▆▇▇▆▄▄▇▇▇▇▇▇█▇▇▇▇▇▇▇█▇▇███▆███▇█████</td></tr><tr><td>datapoints_processed_total</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>rouge1</td><td>▁▇▇▇▇▇▇▇▅▅▇▇█▇▇▇████▇████▇████▇█████████</td></tr><tr><td>rouge2</td><td>▁███████▂▂██████████████████████████████</td></tr><tr><td>rougeL</td><td>▁▇▇▇▇▇█▇▅▅▇▇█▇▇▇████▇████▇████▇█████████</td></tr><tr><td>test_loss</td><td>█▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▂▁▁▁▁</td></tr><tr><td>test_time</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>▅█▆▄▅▆▃▃▁▄▆▃▄▃▃▆▄▄▃▂▅▅▃▁▆▅▄▄▂▁▅▄▂▃▃▄▂▃▃▅</td></tr><tr><td>train_time</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>bleu</td><td>0.19833</td></tr><tr><td>datapoints_processed_total</td><td>519960</td></tr><tr><td>rouge1</td><td>0.64239</td></tr><tr><td>rouge2</td><td>0.48184</td></tr><tr><td>rougeL</td><td>0.59885</td></tr><tr><td>test_loss</td><td>1.0707</td></tr><tr><td>test_time</td><td>4177.46435</td></tr><tr><td>train_loss</td><td>1.51413</td></tr><tr><td>train_time</td><td>35480.92779</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dazzling-planet-115</strong> at: <a href='https://wandb.ai/8667/transformer/runs/riv2dwkf' target=\"_blank\">https://wandb.ai/8667/transformer/runs/riv2dwkf</a><br/> View job at <a href='https://wandb.ai/8667/transformer/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjg2ODg2MjU0/version_details/v10' target=\"_blank\">https://wandb.ai/8667/transformer/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjg2ODg2MjU0/version_details/v10</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230806_025317-riv2dwkf\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline, model, optimizer, loss_function, model_tester = create_model(CONFIG, CONFIG['model_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_train(pipeline, input_text, tokenizer):\n",
    "    input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    input_tokens = input_tokens.to(device)\n",
    "    mask = torch.ones_like(input_tokens)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        probabilities, targets = pipeline.predict_next(input_tokens, mask)\n",
    "        probabilities = functional.log_softmax(probabilities, dim=1)\n",
    "        targets = targets.unsqueeze(0)\n",
    "        print(probabilities.size(), targets.size())\n",
    "    \n",
    "    return loss_function(targets, probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "london is the capital of \n",
    "\"\"\"\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "answer, targets = predict_train(pipeline, input_text, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer, '\\n\\n', targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def predict_next(pipeline, input_text, num_predicted_tokens, tokenizer):\n",
    "    input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    input_tokens = input_tokens[:, :-1].to(device)\n",
    "    pipeline.eval()\n",
    "    \n",
    "    for i in range(num_predicted_tokens):\n",
    "        mask = torch.ones_like(input_tokens)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probabilities = pipeline.next_token_probabilities(input_tokens, mask)\n",
    "        \n",
    "        \n",
    "        answer = probabilities.argsort(dim=-1)[:, -randint(1, 2)].unsqueeze(0)\n",
    "        input_tokens = torch.cat((input_tokens, answer), dim=1)\n",
    "        \n",
    "    return tokenizer.decode(input_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "there is a high chance of\n",
    "\"\"\"\n",
    "num_predicted_tokens = 20\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "answer = predict_next(pipeline, input_text, num_predicted_tokens, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] there is a high chance of consciousness in which a number of people have a particular problem in the world. in the world of the'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    text = batch['input_ids'][5]\n",
    "    break\n",
    "    \n",
    "print(tokenizer.decode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "8defd53a528e9245923bfdc9d5a38f5c3ca09cbc6e92fa1b95a37aeffbb04cd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
