{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from dataset.create_dataset import BookGenerator, create_test_dataset\n",
    "from layers.model import Transformer, AutoregressiveWrapper\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from test_model.test_model import TestModel\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "from time import time\n",
    "from torch.profiler import profile, record_function, ProfilerActivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "CONFIG = {\n",
    "    \"architecture\": \"Transformer\", # Wandb only\n",
    "    \"dataset\": \"books\", #\"wikitext-103-raw-v1\", # Wandb only\n",
    "    \"batch_size\": 12,\n",
    "    \"embedding_size\": 768,\n",
    "    \"max_sequence_length\": 256,\n",
    "    \"number_of_layers\": 12,\n",
    "    \"number_of_heads\": 12,\n",
    "    \"additional_feed_forward_layers\": 0,\n",
    "    \"extention_factor\": 4,\n",
    "    \"dropout_rate\": 0.1,\n",
    "    'train_size': 2**20,\n",
    "    'test_size': 32,\n",
    "    'start_book': 0,\n",
    "    'flash_atten': False,\n",
    "    'num_train_books': 25000,\n",
    "    'save_every': 3000,\n",
    "    'use_mixed_precision': True,\n",
    "    'model_path': None, # \"savepoints/dulcet-serenity-218\",\n",
    "    \n",
    "}\n",
    "CONFIG[\"lr\"] = 0.001 / np.sqrt(CONFIG[\"batch_size\"])\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def log_metrics(test_name, test_dataloader, pipeline, model_tester, train_config, CONFIG):\n",
    "    pipeline.eval()\n",
    "    if test_name in ['books']:\n",
    "        metrics = model_tester.test_model(pipeline, test_dataloader)\n",
    "        test_loss = metrics['loss']\n",
    "        bleu = metrics['bleu']\n",
    "        rouge1 = metrics['rouge1']\n",
    "        rouge2 = metrics['rouge2']\n",
    "        rougeL = metrics['rougeL']\n",
    "\n",
    "        wandb.log({\n",
    "            \"test_loss_\" + test_name: test_loss,\n",
    "            \"bleu_\" + test_name: bleu,\n",
    "            \"rouge1_\" + test_name: rouge1,\n",
    "            \"rouge2_\" + test_name: rouge2,\n",
    "            \"rougeL_\" + test_name: rougeL,\n",
    "        })\n",
    "        \n",
    "    else:\n",
    "        simmilarity_accuracy, simmilarity_score = model_tester.test_simmilarity(pipeline,\n",
    "                                                            test_dataloader, tokenizer)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"simmilarity_accuracy\": simmilarity_accuracy,\n",
    "            \"simmilarity_score\": simmilarity_score,\n",
    "        })\n",
    "\n",
    "    pipeline.train()\n",
    "        \n",
    "\n",
    "def train(CONFIG, pipeline, model, optimizer, loss_function, model_tester, scaler, wandb, dataloaders):\n",
    "    train_config = {\n",
    "        \"test_every\": 2048 // CONFIG[\"batch_size\"],\n",
    "        \"log_traing_metrics_every\": 256 // CONFIG[\"batch_size\"],\n",
    "    }\n",
    "    \n",
    "    book_generator, test_book_dataloader, cloze_dataloader = dataloaders\n",
    "\n",
    "    train_time = 0\n",
    "    \n",
    "    model.train()\n",
    "\n",
    "    batch_num = 0\n",
    "    train_losses = []\n",
    "    tests = ['books', 'simmilarity_score']\n",
    "    test_dataloaders = [test_book_dataloader, cloze_dataloader]\n",
    "    \n",
    "    for i in tqdm(range(CONFIG['num_train_books']), desc=\"Training Progress\"):\n",
    "        train_dataloader = book_generator.next_book()\n",
    "        for batch in train_dataloader:\n",
    "            train_start = time()\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model_output, target = pipeline(input_ids, attention_mask)\n",
    "            with autocast():\n",
    "                loss = loss_function(model_output.transpose(1, 2), target)\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "    \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "\n",
    "            train_time += time() - train_start\n",
    "            batch_num += 1\n",
    "    \n",
    "            if batch_num % train_config[\"log_traing_metrics_every\"] == 0:\n",
    "                wandb.log({\n",
    "                    \"train_loss\": sum(train_losses[-train_config[\"log_traing_metrics_every\"]:]) / train_config[\"log_traing_metrics_every\"],\n",
    "                    \"train_time\": train_time / train_config[\"log_traing_metrics_every\"],\n",
    "                })\n",
    "                train_time = 0\n",
    "            \n",
    "            if batch_num % train_config[\"test_every\"] == 0:\n",
    "                test_start = time()\n",
    "                for i in range(len(tests)):\n",
    "                    log_metrics(tests[i], test_dataloaders[i], pipeline, model_tester, train_config, CONFIG)\n",
    "                wandb.log({\n",
    "                    \"test_time\": time() - test_start,\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(CONFIG, model_path=None):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    number_of_tokens = tokenizer.vocab_size\n",
    "    \n",
    "    model = Transformer(\n",
    "        embedding_size=CONFIG[\"embedding_size\"],\n",
    "        number_of_tokens=number_of_tokens,\n",
    "        number_of_heads=CONFIG[\"number_of_heads\"],\n",
    "        number_of_layers=CONFIG[\"number_of_layers\"],\n",
    "        extention_factor=CONFIG[\"extention_factor\"],\n",
    "        additional_feed_forward_layers=CONFIG[\"additional_feed_forward_layers\"],\n",
    "        dropout_rate=CONFIG[\"dropout_rate\"],\n",
    "        max_sequence_length=CONFIG[\"max_sequence_length\"],\n",
    "        use_flash_att=CONFIG[\"flash_atten\"],\n",
    "    ).to(device).to(torch.float16)\n",
    "    if model_path:\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "        \n",
    "    pipeline = AutoregressiveWrapper(model).to(device)\n",
    "    loss_function = nn.CrossEntropyLoss().to(device).to(torch.float16)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=CONFIG[\"lr\"])\n",
    "    scaler = GradScaler()\n",
    "    model_tester = TestModel(tokenizer, model)\n",
    "\n",
    "    return pipeline, model, optimizer, loss_function, model_tester, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, wandb_run):\n",
    "    PATH = \"savepoints/\" + wandb_run.name\n",
    "    torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskorodumov-work\u001b[0m (\u001b[33m8667\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.10 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\skoro\\VS_projects\\Transformer\\wandb\\run-20230907_200455-ihzwsf7w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/8667/transformer/runs/ihzwsf7w' target=\"_blank\">rural-shape-230</a></strong> to <a href='https://wandb.ai/8667/transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/8667/transformer' target=\"_blank\">https://wandb.ai/8667/transformer</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/8667/transformer/runs/ihzwsf7w' target=\"_blank\">https://wandb.ai/8667/transformer/runs/ihzwsf7w</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb_run = wandb.init(\n",
    "    project=\"transformer\",\n",
    "    tags=[\"long_training_testing\"],\n",
    "    #id=\"sblota90\",\n",
    "    resume=\"allow\",\n",
    "    config=CONFIG\n",
    ")\n",
    "\n",
    "load_path = CONFIG['model_path']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters = 131968314\n",
      "number of trainable parameters = 131968314\n",
      "memory allocated in GB = 0.4916202798485756\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "book_generator = BookGenerator(batch_size=CONFIG[\"batch_size\"],\n",
    "                                max_sequence_length=CONFIG[\"max_sequence_length\"], tokenizer=tokenizer)\n",
    "test_book_dataloader = book_generator.select_test(CONFIG[\"test_size\"])\n",
    "book_generator.skip_books(CONFIG['start_book'])\n",
    "cloze_dataset = create_test_dataset('data.csv')\n",
    "\n",
    "pipeline, model, optimizer, loss_function, model_tester, scaler = create_model(CONFIG, CONFIG[\"model_path\"])\n",
    "num_parameters, num_trainable_parameters, memory_allocated = pipeline.count_parameters() \n",
    "print('number of parameters =', num_parameters)\n",
    "print('number of trainable parameters =', num_trainable_parameters)\n",
    "print('memory allocated in GB =', memory_allocated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|                                             | 0/25000 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.HalfTensor instead (while checking arguments for embedding)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpipeline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_tester\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m     \u001b[49m\u001b[43m[\u001b[49m\u001b[43mbook_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_book_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcloze_dataset\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 57\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(CONFIG, pipeline, model, optimizer, loss_function, model_tester, scaler, wandb, dataloaders)\u001b[0m\n\u001b[0;32m     55\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast():\n\u001b[1;32m---> 57\u001b[0m     model_output, target \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_function(model_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m), target)\n\u001b[0;32m     60\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\users\\skoro\\vs_projects\\virtualenvs\\torchvenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\VS_projects\\Transformer\\layers\\model.py:70\u001b[0m, in \u001b[0;36mAutoregressiveWrapper.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     68\u001b[0m inp, target \u001b[38;5;241m=\u001b[39m x[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], x[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m     69\u001b[0m mask \u001b[38;5;241m=\u001b[39m mask[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 70\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, target\n",
      "File \u001b[1;32mc:\\users\\skoro\\vs_projects\\virtualenvs\\torchvenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\VS_projects\\Transformer\\layers\\model.py:40\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[1;32m---> 40\u001b[0m     token_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m     positional_encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_encoding(token_embeddings)\n\u001b[0;32m     42\u001b[0m     positional_encoding_normalized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_normalization(positional_encoding)\n",
      "File \u001b[1;32mc:\\users\\skoro\\vs_projects\\virtualenvs\\torchvenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\VS_projects\\Transformer\\layers\\embeddings.py:9\u001b[0m, in \u001b[0;36mTokenEmbedding.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m----> 9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\skoro\\vs_projects\\virtualenvs\\torchvenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\users\\skoro\\vs_projects\\virtualenvs\\torchvenv\\lib\\site-packages\\torch\\nn\\modules\\sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\skoro\\vs_projects\\virtualenvs\\torchvenv\\lib\\site-packages\\torch\\nn\\functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.HalfTensor instead (while checking arguments for embedding)"
     ]
    }
   ],
   "source": [
    "train(CONFIG, pipeline, model, optimizer, loss_function, model_tester, scaler, wandb,\n",
    "     [book_generator, test_book_dataloader, cloze_dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"savepoints/\" + wandb_run.name\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "When you want to travel around the city, you can take a colorful car called a\n",
    "\"\"\"\n",
    "num_predicted_tokens = 20\n",
    "k = 3\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "start = time()\n",
    "answer = pipeline.predict_next(input_text, tokenizer, num_predicted_tokens, k)\n",
    "print(.time() - start)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def predict_next(pipeline, input_text, num_predicted_tokens, tokenizer):\n",
    "    input_tokens = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "    input_tokens = input_tokens[:, :-1].to(device)\n",
    "    \n",
    "    for i in range(num_predicted_tokens):\n",
    "        mask = torch.ones_like(input_tokens)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probabilities = pipeline.next_token_probabilities(input_tokens, mask)\n",
    "        \n",
    "        answer = probabilities.argsort(dim=-1)[:, -randint(1, 2)].unsqueeze(0)\n",
    "        input_tokens = torch.cat((input_tokens, answer), dim=1)\n",
    "        \n",
    "    return tokenizer.decode(input_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "london is the capital of\n",
    "\"\"\"\n",
    "num_predicted_tokens = 40\n",
    "\n",
    "answer = predict_next(pipeline, input_text, num_predicted_tokens, tokenizer)\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simmilarity_accuracy, simmilarity_score = model_tester.test_simmilarity(pipeline,\n",
    "                                                            cloze_dataset, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "GPU comparison\n",
    "1. GOOGLE COLAB\n",
    "-100 CU are given for 10 dollars\n",
    "-V100, 32 GB, 5 CU per hour, 20 hours, 706 DP, 14120 DP total, 1412 DP per dollar\n",
    "-A100, 40 GB, 15 CU per hour, 6.5 hours, 2179 DP, 14163 DP total, 1416 DP per dollar\n",
    "\n",
    "2. VAST.AI\n",
    "-RTX 4090, 24 GB, 0.4 dollars per hour, 1720 DP, 4300 DP per dollar, 177% while training on 4 GPUs\n",
    "-RTX 3080, 10 GB, 0.117 dollars per hour, 1022 DP, 8717 DP per dollar, 177% while training on 4 GPUs\n",
    "-RTX 3090, 24 GB, 0.23 dollars per hour, 1071 DP, 4652 DP per dollar, 177% while training on 4 GPUs\n",
    "-A5000, 24 GB, 0.22 dollars per hour, 1061 DP, 4686 DP per dollar, 389% while training on 4 GPUs\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO\n",
    "optimize\n",
    "-understand time wasting\n",
    "-choose batch size\n",
    "\n",
    "\n",
    "beam search\n",
    "\n",
    "new dataset\n",
    "-find new dataset for big training\n",
    "-make some analysis\n",
    "-avg, mean, each percentile sequence length\n",
    "-find info in the net\n",
    "\n",
    "model parameters\n",
    "-max sequence length\n",
    "-estimate batch size\n",
    "-choose parameters for n GB VRAM\n",
    "-choose model parameters for 5-6 GB model\n",
    "\n",
    "filter model\n",
    "-\n",
    "\n",
    "new conversation dataset \n",
    "-analyse\n",
    "-understand how to train and test conversation model\n",
    "-try to train\n",
    "\n",
    "metrics\n",
    "-create metrics for conversation\n",
    "\n",
    "multiple GPUs\n",
    "-write code to run model on several gpus\n",
    "-try to run model on vast.ai\n",
    "-also model must be saved per epoch\n",
    "\n",
    "site\n",
    "-create django project with model which is placed on google colab\n",
    "-create \n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r'ServiceKey_GoogleCloud.json'\n",
    "\n",
    "storage_client = storage.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\skoro\\AppData\\Local\\Temp\\ipykernel_20760\\477664455.py:6: DeprecationWarning: Assignment to 'Bucket.location' is deprecated, as it is only valid before the bucket is created. Instead, pass the location to `Bucket.create`.\n",
      "  bucket.location = 'US' # Taiwan\n"
     ]
    },
    {
     "ename": "Forbidden",
     "evalue": "403 POST https://storage.googleapis.com/storage/v1/b?project=gpt-saves&prettyPrint=false: The billing account for the owning project is disabled in state absent",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mForbidden\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m bucket \u001b[38;5;241m=\u001b[39m storage_client\u001b[38;5;241m.\u001b[39mbucket(bucket_name)\n\u001b[0;32m      6\u001b[0m bucket\u001b[38;5;241m.\u001b[39mlocation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUS\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m# Taiwan\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m bucket \u001b[38;5;241m=\u001b[39m \u001b[43mstorage_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_bucket\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# returns Bucket object\u001b[39;00m\n\u001b[0;32m      9\u001b[0m pprint(\u001b[38;5;28mvars\u001b[39m(bucket))\n",
      "File \u001b[1;32mc:\\users\\skoro\\vs_projects\\virtualenvs\\torchvenv\\lib\\site-packages\\google\\cloud\\storage\\client.py:961\u001b[0m, in \u001b[0;36mClient.create_bucket\u001b[1;34m(self, bucket_or_name, requester_pays, project, user_project, location, data_locations, predefined_acl, predefined_default_object_acl, timeout, retry)\u001b[0m\n\u001b[0;32m    958\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data_locations \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    959\u001b[0m     properties[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomPlacementConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataLocations\u001b[39m\u001b[38;5;124m\"\u001b[39m: data_locations}\n\u001b[1;32m--> 961\u001b[0m api_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post_resource\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/b\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproperties\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_target_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m bucket\u001b[38;5;241m.\u001b[39m_set_properties(api_response)\n\u001b[0;32m    971\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bucket\n",
      "File \u001b[1;32mc:\\users\\skoro\\vs_projects\\virtualenvs\\torchvenv\\lib\\site-packages\\google\\cloud\\storage\\client.py:625\u001b[0m, in \u001b[0;36mClient._post_resource\u001b[1;34m(self, path, data, query_params, headers, timeout, retry, _target_object)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_post_resource\u001b[39m(\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    566\u001b[0m     path,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    572\u001b[0m     _target_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    573\u001b[0m ):\n\u001b[0;32m    574\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Helper for bucket / blob methods making API 'POST' calls.\u001b[39;00m\n\u001b[0;32m    575\u001b[0m \n\u001b[0;32m    576\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;124;03m            If the bucket is not found.\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 625\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPOST\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    630\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    633\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_target_object\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_target_object\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\skoro\\vs_projects\\virtualenvs\\torchvenv\\lib\\site-packages\\google\\cloud\\storage\\_http.py:72\u001b[0m, in \u001b[0;36mConnection.api_request\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m retry:\n\u001b[0;32m     71\u001b[0m         call \u001b[38;5;241m=\u001b[39m retry(call)\n\u001b[1;32m---> 72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\skoro\\vs_projects\\virtualenvs\\torchvenv\\lib\\site-packages\\google\\api_core\\retry.py:349\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    345\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    346\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[0;32m    348\u001b[0m )\n\u001b[1;32m--> 349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\skoro\\vs_projects\\virtualenvs\\torchvenv\\lib\\site-packages\\google\\api_core\\retry.py:191\u001b[0m, in \u001b[0;36mretry_target\u001b[1;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sleep \u001b[38;5;129;01min\u001b[39;00m sleep_generator:\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 191\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# This function explicitly must deal with broad exceptions.\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\users\\skoro\\vs_projects\\virtualenvs\\torchvenv\\lib\\site-packages\\google\\cloud\\_http\\__init__.py:494\u001b[0m, in \u001b[0;36mJSONConnection.api_request\u001b[1;34m(self, method, path, query_params, data, content_type, headers, api_base_url, api_version, expect_json, _target_object, timeout, extra_api_info)\u001b[0m\n\u001b[0;32m    482\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[0;32m    483\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    484\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m     extra_api_info\u001b[38;5;241m=\u001b[39mextra_api_info,\n\u001b[0;32m    491\u001b[0m )\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m200\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m300\u001b[39m:\n\u001b[1;32m--> 494\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_http_response(response)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expect_json \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent:\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[1;31mForbidden\u001b[0m: 403 POST https://storage.googleapis.com/storage/v1/b?project=gpt-saves&prettyPrint=false: The billing account for the owning project is disabled in state absent"
     ]
    }
   ],
   "source": [
    "bucket_name = 'uu_gpt'\n",
    "\n",
    "# create a new bucket\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "\n",
    "bucket.location = 'US' # Taiwan\n",
    "bucket = storage_client.create_bucket(bucket) # returns Bucket object\n",
    "\n",
    "pprint(vars(bucket))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "8defd53a528e9245923bfdc9d5a38f5c3ca09cbc6e92fa1b95a37aeffbb04cd1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
